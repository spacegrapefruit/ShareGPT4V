{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4fa4aa",
   "metadata": {},
   "source": [
    "# ShareGPT4V - LoRA Fine-tuning\n",
    "\n",
    "1. Set up the required libraries\n",
    "2. Prepare the model and dataset\n",
    "3. Configure LoRA\n",
    "4. Fine-tune the model\n",
    "5. Save the LoRA weights\n",
    "6. Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f60883",
   "metadata": {},
   "source": [
    "## 1. Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374c702b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# import transformers\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from share4v.model import Share4VLlamaForCausalLM\n",
    "from share4v.constants import DEFAULT_IMAGE_TOKEN\n",
    "from share4v.mm_utils import tokenizer_image_token\n",
    "from share4v.model.builder import load_pretrained_model\n",
    "\n",
    "from share4v.train.train import get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3\n",
    "\n",
    "# Set up basic configurations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489f246",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be68c384-3e5b-4503-b949-f88e60024212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share4v-7b\n",
      "{'device_map': 'auto', 'torch_dtype': torch.float32}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vision tower from Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Lin-Chen/ShareGPT4V-7B\"\n",
    "model_name = \"share4v-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name, False, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305e4c23-2263-4da7-8fdc-b5af76d9a33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2386f3",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a21ed4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from share4v.train.train import (\n",
    "    LazySupervisedDataset, \n",
    "    DataArguments, \n",
    "    DataCollatorForSupervisedDataset\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    data_path=\"../data/example_training.json\",\n",
    "    lazy_preprocess=False, # Typically TRUE\n",
    "    is_multimodal=True,\n",
    "    image_folder=\"../data\",\n",
    "    image_aspect_ratio=\"square\"\n",
    ")\n",
    "data_args.image_processor = image_processor\n",
    "data_args.mm_use_im_start_end = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08e0758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 2 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = LazySupervisedDataset(\n",
    "    data_path=data_args.data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    data_args=data_args\n",
    ")\n",
    "print(f\"Dataset created with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a4eb1",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46435f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example data:\n",
      "--------------------------------------------------\n",
      "Keys in example: ['input_ids', 'labels', 'image']\n",
      "Input IDs shape: torch.Size([131])\n",
      "Labels shape: torch.Size([131])\n",
      "Image tensor shape: torch.Size([3, 336, 336])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example = dataset[0]\n",
    "print(\"\\nExample data:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Keys in example: {list(example.keys())}\")\n",
    "if \"input_ids\" in example:\n",
    "    print(f\"Input IDs shape: {example['input_ids'].shape}\")\n",
    "if \"labels\" in example:\n",
    "    print(f\"Labels shape: {example['labels'].shape}\")\n",
    "if \"image\" in example:\n",
    "    print(f\"Image tensor shape: {example['image'].shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9654c794-2fb1-4b07-b1ef-86209a847e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a curious user and an artificial intelligence assistant . The assistant gives helpful , detailed , and pol ite answers to the user ' s questions . US ER :  \n",
      "Error decoding token: tensor(-200)\n",
      " <0x0A> What ' s happening in this image ? A SS IST ANT : This image shows Walter White and J esse P ink man from the TV series Bre aking Bad . They appear to be in their mobile m eth lab , we aring protect ive yellow ha z mat su its , which is an icon ic scene from the show . The two characters are likely engaged in cook ing m eth am ph et am ine , which is the central activity dep icted throughout the series . </s> "
     ]
    }
   ],
   "source": [
    "for i in example['input_ids']:\n",
    "    try:\n",
    "        print(tokenizer.decode(i), end=\" \")\n",
    "    except:\n",
    "        print()\n",
    "        print(\"Error decoding token:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85fbb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the original DataCollatorForSupervisedDataset from train.py\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cf447",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226f477a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the linear layers for LoRA\n",
    "# Copyed from train.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find all linear layer names in the model that are suitable for LoRA.\n",
    "    Excludes multimodal components (vision tower, mm_projector, etc.)\n",
    "    \"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa73c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_model_for_lora(\n",
    "    model, \n",
    "    lora_r=64,            # LoRA rank - lower means fewer parameters, higher means more capacity\n",
    "    lora_alpha=16,        # LoRA alpha - scaling factor (usually 2x to 4x of rank)\n",
    "    lora_dropout=0.05,    # LoRA dropout - regularization to prevent overfitting\n",
    "    bias=\"none\",          # Whether to train bias parameters (\"none\", \"all\", or \"lora_only\")\n",
    "    target_modules=None,  # Which modules to apply LoRA to. If None, will find all linear layers\n",
    "    task_type=\"CAUSAL_LM\" # Task type for LoRA configuration\n",
    "):\n",
    "    if target_modules is None:\n",
    "        target_modules = find_all_linear_names(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha, \n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=lora_dropout, \n",
    "        bias=bias,\n",
    "        task_type=task_type,\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return peft_model\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n",
    "    return trainable_params, all_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8966818e-147c-4e00-87f6-5884c1738286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b32c845-8582-4d3b-a7b2-8ab2966a8d93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'down_proj', 'gate_proj', 'o_proj', 'v_proj', 'k_proj', 'up_proj']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7e325c4-7b74-4f00-8baf-37930ee1ed59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 42336256 || all params: 7105239040 || trainable%: 0.60%\n",
      "trainable params: 39845888 || all params: 7102748672 || trainable%: 0.56%\n",
      "trainable params: 9961472 || all params: 7072864256 || trainable%: 0.14%\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "# Store results for comparison\n",
    "config_results = []\n",
    "\n",
    "model_config1 = copy.deepcopy(model)\n",
    "model_config1 = prepare_model_for_lora(model_config1, lora_r=64, lora_alpha=16)\n",
    "\n",
    "trainable1, total1 = print_trainable_parameters(model_config1)\n",
    "del model_config1\n",
    "config_results.append({\"name\": \"Default (r=64, alpha=16)\", \"trainable\": trainable1, \"total\": total1})\n",
    "\n",
    "model_config2 = copy.deepcopy(model)\n",
    "model_config2 = prepare_model_for_lora(model_config2, lora_r=16, lora_alpha=32)\n",
    "trainable2, total2 = print_trainable_parameters(model_config2)\n",
    "del model_config2\n",
    "config_results.append({\"name\": \"Low rank (r=16, alpha=32)\", \"trainable\": trainable2, \"total\": total2})\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\"]  # Only attention query and value projections\n",
    "model_config3 = copy.deepcopy(model)\n",
    "model_config3 = prepare_model_for_lora(model_config3, lora_r=64, lora_alpha=16, target_modules=target_modules)\n",
    "trainable3, total3 = print_trainable_parameters(model_config3)\n",
    "del model_config3\n",
    "config_results.append({\"name\": \"Attention only (r=64, alpha=16)\", \"trainable\": trainable3, \"total\": total3})\n",
    "\n",
    "model_config4 = copy.deepcopy(model)\n",
    "model_config4 = prepare_model_for_lora(model_config4, lora_r=16, lora_alpha=16, target_modules=target_modules)\n",
    "trainable4, total4 = print_trainable_parameters(model_config4)\n",
    "del model_config4\n",
    "config_results.append({\"name\": \"Attention only (r=16, alpha=16)\", \"trainable\": trainable4, \"total\": total4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97fa4e24-8101-469c-a782-95da7c2a757a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Configuration Comparison ##\n",
      "----------------------------------------\n",
      "Configuration                            Trainable    % of Model  \n",
      "----------------------------------------\n",
      "Default (r=64, alpha=16)                 169,345,024  2.3415%\n",
      "Low rank (r=16, alpha=32)                42,336,256   0.5958%\n",
      "Attention only (r=64, alpha=16)          39,845,888   0.5610%\n",
      "Attention only (r=16, alpha=16)          9,961,472    0.1408%\n"
     ]
    }
   ],
   "source": [
    "# Print comparison table\n",
    "print(\"## Configuration Comparison ##\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"{'Configuration':<40} {'Trainable':<12} {'% of Model':<12}\")\n",
    "print(\"----------------------------------------\")\n",
    "for config in config_results:\n",
    "    print(f\"{config['name']:<40} {config['trainable']:<12,d} {(config['trainable']/config['total']*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c44dcc",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ae234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model from config 4\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "model = prepare_model_for_lora(model, lora_r=16, lora_alpha=16, target_modules=target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c09db4-4b8b-4e70-930f-56f17ae5e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from share4v.train.share4v_trainer import Share4VTrainer\n",
    "from share4v.train.train import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d51154f-9d3a-4734-bbfa-7ee80413931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_share4v_output\",  # Output directory\n",
    "    num_train_epochs=1,                  # Number of training epochs\n",
    "    # per_device_train_batch_size=4,       # Batch size per device\n",
    "    gradient_accumulation_steps=4,       # Number of update steps to accumulate gradients for\n",
    "    learning_rate=2e-5,                  # Learning rate\n",
    "    weight_decay=0.01,                   # Weight decay\n",
    "    save_steps=5,                      # Save every 500 steps\n",
    "    save_total_limit=3,                  # Keep only the 3 most recent checkpoints\n",
    "    report_to=None,                      # Disable reporting to wandb etc.\n",
    "    remove_unused_columns=False,         # Keep all columns\n",
    "    log_level=\"info\",                    # Logging level\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    # fp16=True,                           # Use mixed precision\n",
    "    lora_enable=True,                    # Enable LoRA training\n",
    "    group_by_modality_length=False       # Don't group by modality length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5194a93f-2de1-4a30-b49a-b66cbfbe6adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8af253-15fc-4fa0-afa6-8599ea083976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a fix for: model has strange 'hf_device_map' property after lora\n",
    "\n",
    "# model.hf_device_map\n",
    "# [device for device in set(model.hf_device_map.values()) if device not in [\"cpu\", \"disk\"]]\n",
    "model.hf_device_map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae955f2d-bc64-4078-b818-69399c759864",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Share4VTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce73056-ffe5-4859-a46b-e606f395febc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.is_model_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36504b3",
   "metadata": {},
   "source": [
    "## 6. Train\n",
    "\n",
    "Now we'll start the training process and save the LoRA weights after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02cdbf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 9,961,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7dbdac3-d92e-4e3b-84b2-38d918449b6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\n",
      "  Using cached deepspeed-0.16.7-py3-none-any.whl\n",
      "Requirement already satisfied: einops in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (0.6.1)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (24.2)\n",
      "Requirement already satisfied: psutil in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (7.0.0)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (2.11.2)\n",
      "Requirement already satisfied: torch in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (2.0.1+cpu)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from torch->deepspeed) (3.18.0)\n",
      "Requirement already satisfied: sympy in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from torch->deepspeed) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from torch->deepspeed) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of share4v: Local version label can only be used with `==` or `!=` operators\n",
      "    torch (>=2.0.1+cpu,<3.0.0)\n",
      "           ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: py-cpuinfo, hjson, ninja, msgpack, deepspeed\n",
      "Successfully installed deepspeed-0.16.7 hjson-3.1.0 msgpack-1.1.0 ninja-1.11.1.4 py-cpuinfo-9.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Quick install for no notebook restarting sr\n",
    "# !pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca461934-8840-4061-a999-bae1e2e3dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weights\n",
    "output_dir = \"share4v_lora_weights\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a53487fb-8299-451f-a6bb-897c6002ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save LoRA state dict\n",
    "lora_state_dict = get_peft_state_maybe_zero_3(\n",
    "    model.named_parameters(), bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345b6086-22c1-48c3-83d9-89bc389a8f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lora_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec41a764-dc8d-4623-b720-5f56282e2a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_state_dict[\"base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef9c9bdd-0544-44ec-8e9a-7b37d15c9e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_state_dict[\"base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30744448-495f-4b24-aeb7-fa13a11408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract non-LoRA trainable weights (like special token embeddings)\n",
    "non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "    model.named_parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "881dec88-7af6-4f0c-a35a-f647688b01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in share4v_lora_weights/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA weights saved to share4v_lora_weights\n"
     ]
    }
   ],
   "source": [
    "# Save the model configuration and weights\n",
    "model.config.save_pretrained(output_dir)\n",
    "model.save_pretrained(output_dir, state_dict=lora_state_dict)\n",
    "torch.save(non_lora_state_dict, os.path.join(output_dir, 'non_lora_trainables.bin'))\n",
    "\n",
    "print(f\"LoRA weights saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-poetry-kernel",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "conda-base-poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
