{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4fa4aa",
   "metadata": {},
   "source": [
    "# ShareGPT4V - LoRA Fine-tuning\n",
    "\n",
    "1. Set up the required libraries\n",
    "2. Prepare the model and dataset\n",
    "3. Configure LoRA\n",
    "4. Fine-tune the model\n",
    "5. Save the LoRA weights\n",
    "6. Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f60883",
   "metadata": {},
   "source": [
    "## 1. Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# import transformers\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from share4v.model import Share4VLlamaForCausalLM\n",
    "from share4v.constants import DEFAULT_IMAGE_TOKEN\n",
    "from share4v.mm_utils import tokenizer_image_token\n",
    "from share4v.model.builder import load_pretrained_model\n",
    "\n",
    "from share4v.train.train import get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3\n",
    "\n",
    "# Set up basic configurations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489f246",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Lin-Chen/ShareGPT4V-7B\"\n",
    "model_name = \"share4v-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name, False, False\n",
    ")\n",
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2386f3",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from share4v.train.train import (\n",
    "    LazySupervisedDataset, \n",
    "    DataArguments, \n",
    "    DataCollatorForSupervisedDataset\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    data_path=\"./data/example_training.json\",\n",
    "    lazy_preprocess=False, # Typically TRUE\n",
    "    is_multimodal=True,\n",
    "    image_folder=\"/home/justas/ShareGPT4V/data\",\n",
    "    image_aspect_ratio=\"square\"\n",
    ")\n",
    "data_args.image_processor = image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LazySupervisedDataset(\n",
    "    data_path=data_args.data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    data_args=data_args\n",
    ")\n",
    "print(f\"Dataset created with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a4eb1",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46435f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "print(\"\\nExample data:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Keys in example: {list(example.keys())}\")\n",
    "if \"input_ids\" in example:\n",
    "    print(f\"Input IDs shape: {example['input_ids'].shape}\")\n",
    "if \"labels\" in example:\n",
    "    print(f\"Labels shape: {example['labels'].shape}\")\n",
    "if \"image\" in example:\n",
    "    print(f\"Image tensor shape: {example['image'].shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of 1 dataset entry with image and the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the original DataCollatorForSupervisedDataset from train.py\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cf447",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the linear layers for LoRA\n",
    "# Copyed from train.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find all linear layer names in the model that are suitable for LoRA.\n",
    "    Excludes multimodal components (vision tower, mm_projector, etc.)\n",
    "    \"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa73c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_lora(\n",
    "    model, \n",
    "    lora_r=64,            # LoRA rank - lower means fewer parameters, higher means more capacity\n",
    "    lora_alpha=16,        # LoRA alpha - scaling factor (usually 2x to 4x of rank)\n",
    "    lora_dropout=0.05,    # LoRA dropout - regularization to prevent overfitting\n",
    "    bias=\"none\",          # Whether to train bias parameters (\"none\", \"all\", or \"lora_only\")\n",
    "    target_modules=None,  # Which modules to apply LoRA to. If None, will find all linear layers\n",
    "    task_type=\"CAUSAL_LM\" # Task type for LoRA configuration\n",
    "):\n",
    "    if target_modules is None:\n",
    "        target_modules = find_all_linear_names(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha, \n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=lora_dropout, \n",
    "        bias=bias,\n",
    "        task_type=task_type,\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return peft_model\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13658f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for comparison\n",
    "config_results = []\n",
    "\n",
    "model_config1 = prepare_model_for_lora(model, lora_r=64, lora_alpha=16)\n",
    "trainable1, total1 = print_trainable_parameters(model_config1)\n",
    "del model_config1\n",
    "config_results.append({\"name\": \"Default (r=64, alpha=16)\", \"trainable\": trainable1, \"total\": total1})\n",
    "print(\"\\n\")\n",
    "\n",
    "model_config2 = prepare_model_for_lora(model, lora_r=16, lora_alpha=32)\n",
    "trainable2, total2 = print_trainable_parameters(model_config2)\n",
    "del model_config2\n",
    "config_results.append({\"name\": \"Low rank (r=16, alpha=32)\", \"trainable\": trainable2, \"total\": total2})\n",
    "print(\"\\n\")\\\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\"]  # Only attention query and value projections\n",
    "model_config3 = prepare_model_for_lora(model, lora_r=64, lora_alpha=16, target_modules=target_modules)\n",
    "trainable3, total3 = print_trainable_parameters(model_config3)\n",
    "del model_config3\n",
    "config_results.append({\"name\": \"Attention only (r=64, alpha=16)\", \"trainable\": trainable3, \"total\": total3})\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\"]  # Only attention query and value projections\n",
    "model_config4 = prepare_model_for_lora(model, lora_r=16, lora_alpha=16, target_modules=target_modules)\n",
    "trainable4, total4 = print_trainable_parameters(model_config4)\n",
    "del model_config4\n",
    "config_results.append({\"name\": \"Attention only (r=16, alpha=16)\", \"trainable\": trainable4, \"total\": total4})\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Print comparison table\n",
    "print(\"## Configuration Comparison ##\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"{'Configuration':<40} {'Trainable':<12} {'% of Model':<12}\")\n",
    "print(\"----------------------------------------\")\n",
    "for config in config_results:\n",
    "    print(f\"{config['name']:<40} {config['trainable']:<12,d} {(config['trainable']/config['total']*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c44dcc",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model from config 4\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "model = prepare_model_for_lora(model, lora_r=16, lora_alpha=16, target_modules=target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from share4v.train.share4v_trainer import Share4VTrainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_share4v_output\",  # Output directory\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    per_device_train_batch_size=4,       # Batch size per device\n",
    "    gradient_accumulation_steps=4,       # Number of update steps to accumulate gradients for\n",
    "    learning_rate=2e-5,                  # Learning rate\n",
    "    weight_decay=0.01,                   # Weight decay\n",
    "    save_steps=5,                      # Save every 500 steps\n",
    "    save_total_limit=3,                  # Keep only the 3 most recent checkpoints\n",
    "    report_to=None,                      # Disable reporting to wandb etc.\n",
    "    remove_unused_columns=False,         # Keep all columns\n",
    "    log_level=\"info\",                    # Logging level\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    fp16=True,                           # Use mixed precision\n",
    "    lora_enable=True,                    # Enable LoRA training\n",
    "    group_by_modality_length=False       # Don't group by modality length\n",
    ")\n",
    "\n",
    "trainer = Share4VTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36504b3",
   "metadata": {},
   "source": [
    "## 6. Train\n",
    "\n",
    "Now we'll start the training process and save the LoRA weights after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weights\n",
    "output_dir = \"share4v_lora_weights\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract and save LoRA state dict\n",
    "lora_state_dict = get_peft_state_maybe_zero_3(\n",
    "    model.named_parameters(), bias=\"none\"\n",
    ")\n",
    "\n",
    "# Extract non-LoRA trainable weights (like special token embeddings)\n",
    "non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "    model.named_parameters()\n",
    ")\n",
    "\n",
    "# Save the model configuration and weights\n",
    "model.config.save_pretrained(output_dir)\n",
    "model.save_pretrained(output_dir, state_dict=lora_state_dict)\n",
    "torch.save(non_lora_state_dict, os.path.join(output_dir, 'non_lora_trainables.bin'))\n",
    "\n",
    "print(f\"LoRA weights saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "share4v-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
