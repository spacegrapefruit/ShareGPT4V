{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5658e08-3c3b-4927-a522-a2cf83c06e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb68750b-b6e9-4787-8bf4-6869673d299c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = \"\"\"\n",
    "python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
    "    --model_name_or_path \"Lin-Chen/ShareGPT4V-7B\" \\\n",
    "    --version \"vicuna_v1\" \\  # Or other appropriate conversation template version\n",
    "    --data_path \"/home/jupyter/ShareGPT4V/train_samples_tuning.json\" \\\n",
    "    --output_dir \"path/to/your/output_lora_adapters\" \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 2e-4 \\ # This LR will be for any non-LoRA params if tuned. LoRA params scale with alpha/r.\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    # TODO --tf32 False \\ # Set to True if using Ampere GPUs or newer for speed\n",
    "    # TODO --bf16 True \\  # Or --fp16 True, depending on your hardware. BF16 is generally preferred.\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    \\\n",
    "    # --- LoRA Specific Arguments ---\n",
    "    --lora_enable True \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 16 \\ # Or lora_r value\n",
    "    --lora_dropout 0.05 \\\n",
    "    # --lora_bias \"none\" # Default is \"none\"\n",
    "    \\\n",
    "    # --- Multimodal Specific Arguments (if applicable) ---\n",
    "    --vision_tower \"Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\" \\\n",
    "    --image_folder \"/home/jupyter/art_images_data/images\" \\\n",
    "    --is_multimodal True \\\n",
    "    --image_aspect_ratio 'pad' \\\n",
    "    --mm_projector_type 'mlp2x_gelu' \\ # Or other projector type\n",
    "    # TODO --tune_mm_mlp_adapter True \\ # Option to tune the projector alongside LoRA\n",
    "    # --tune_vision_tower True # Option to tune the vision tower (can be memory intensive)\n",
    "    # --vision_tower_lr 2e-5 # If tuning vision tower, specify its LR\n",
    "    # --mm_projector_lr 2e-5 # If tuning MLP adapter, specify its LR\n",
    "    \\\n",
    "    # --- QLoRA Specific Arguments (if using quantization) ---\n",
    "    --bits 4 \\\n",
    "    # TODO --double_quant True \\\n",
    "    --quant_type \"nf4\" \\\n",
    "    # TODO extra params\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7696d2bc-d36f-4c64-8711-c8d1cc7badc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = cmd.split(\"    \")\n",
    "lines = [\n",
    "    re.sub(r\"\\s*#.*\", \"\", l).strip().strip(\"\\\\\").strip()\n",
    "    for l in lines\n",
    "]\n",
    "lines = [l for l in lines if l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b95d1a7-1a49-4d26-9896-5ac64147652d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
      "    --model_name_or_path \"Lin-Chen/ShareGPT4V-7B\" \\\n",
      "    --version \"vicuna_v1\" \\\n",
      "    --data_path \"/home/jupyter/ShareGPT4V/train_samples_tuning.json\" \\\n",
      "    --output_dir \"path/to/your/output_lora_adapters\" \\\n",
      "    --num_train_epochs 3 \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 8 \\\n",
      "    --learning_rate 2e-4 \\\n",
      "    --weight_decay 0. \\\n",
      "    --warmup_ratio 0.03 \\\n",
      "    --lr_scheduler_type \"cosine\" \\\n",
      "    --logging_steps 1 \\\n",
      "    --model_max_length 2048 \\\n",
      "    --gradient_checkpointing True \\\n",
      "    --lazy_preprocess True \\\n",
      "    --lora_enable True \\\n",
      "    --lora_r 64 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --vision_tower \"Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\" \\\n",
      "    --image_folder \"/home/jupyter/art_images_data/images\" \\\n",
      "    --is_multimodal True \\\n",
      "    --image_aspect_ratio 'pad' \\\n",
      "    --mm_projector_type 'mlp2x_gelu' \\\n",
      "    --bits 4 \\\n",
      "    --quant_type \"nf4\"\n"
     ]
    }
   ],
   "source": [
    "print(\"!\" + \" \\\\\\n    \".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a074194-f8af-44a2-bf43-21a004e47aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-19 23:33:42,729] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.97s/it]\n",
      "Adding LoRA adapters...\n",
      "Load vision tower from Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\n",
      "Total parameters: 3,854,391,296\n",
      "Trainable parameters: 39,976,960\n",
      "Frozen parameters: 3,814,414,336\n",
      "Trainable%: 1.04%\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:427: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1301: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  new_forward = torch.cuda.amp.autocast(dtype=torch.float16)(model_forward_func)\n",
      "  0%|                                                  | 0/1126 [00:00<?, ?it/s]/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.7837, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.8455, 'learning_rate': 1.1764705882352942e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6921, 'learning_rate': 1.7647058823529414e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6778, 'learning_rate': 2.3529411764705884e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.8237, 'learning_rate': 2.9411764705882354e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7296, 'learning_rate': 3.529411764705883e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.7434, 'learning_rate': 4.11764705882353e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.4857, 'learning_rate': 4.705882352941177e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.5767, 'learning_rate': 5.294117647058824e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5636, 'learning_rate': 5.882352941176471e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.6966, 'learning_rate': 6.470588235294118e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5538, 'learning_rate': 7.058823529411765e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.3611, 'learning_rate': 7.647058823529411e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5755, 'learning_rate': 8.23529411764706e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.513, 'learning_rate': 8.823529411764706e-05, 'epoch': 0.03}          \n",
      "{'loss': 3.2134, 'learning_rate': 9.411764705882353e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.2027, 'learning_rate': 0.0001, 'epoch': 0.03}                        \n",
      "{'loss': 3.1591, 'learning_rate': 0.00010588235294117647, 'epoch': 0.03}        \n",
      "{'loss': 2.9361, 'learning_rate': 0.00011176470588235294, 'epoch': 0.03}        \n",
      "{'loss': 3.0375, 'learning_rate': 0.00011764705882352942, 'epoch': 0.04}        \n",
      "{'loss': 2.9458, 'learning_rate': 0.0001235294117647059, 'epoch': 0.04}         \n",
      "{'loss': 2.7573, 'learning_rate': 0.00012941176470588237, 'epoch': 0.04}        \n",
      "{'loss': 2.7811, 'learning_rate': 0.00013529411764705884, 'epoch': 0.04}        \n",
      "{'loss': 2.5658, 'learning_rate': 0.0001411764705882353, 'epoch': 0.04}         \n",
      "{'loss': 2.829, 'learning_rate': 0.00014705882352941178, 'epoch': 0.04}         \n",
      "{'loss': 2.6226, 'learning_rate': 0.00015294117647058822, 'epoch': 0.05}        \n",
      "{'loss': 2.9252, 'learning_rate': 0.0001588235294117647, 'epoch': 0.05}         \n",
      "{'loss': 2.925, 'learning_rate': 0.0001647058823529412, 'epoch': 0.05}          \n",
      "{'loss': 2.4459, 'learning_rate': 0.00017058823529411766, 'epoch': 0.05}        \n",
      "{'loss': 2.636, 'learning_rate': 0.00017647058823529413, 'epoch': 0.05}         \n",
      "{'loss': 2.4842, 'learning_rate': 0.0001823529411764706, 'epoch': 0.06}         \n",
      "{'loss': 2.3809, 'learning_rate': 0.00018823529411764707, 'epoch': 0.06}        \n",
      "{'loss': 2.332, 'learning_rate': 0.00019411764705882354, 'epoch': 0.06}         \n",
      "{'loss': 2.2661, 'learning_rate': 0.0002, 'epoch': 0.06}                        \n",
      "{'loss': 2.5149, 'learning_rate': 0.00019999958616790135, 'epoch': 0.06}        \n",
      "{'loss': 2.2726, 'learning_rate': 0.00019999834467503052, 'epoch': 0.06}        \n",
      "{'loss': 2.0957, 'learning_rate': 0.00019999627553166294, 'epoch': 0.07}        \n",
      "{'loss': 2.3054, 'learning_rate': 0.0001999933787549241, 'epoch': 0.07}         \n",
      "{'loss': 1.8899, 'learning_rate': 0.00019998965436878962, 'epoch': 0.07}        \n",
      "{'loss': 2.2317, 'learning_rate': 0.00019998510240408496, 'epoch': 0.07}        \n",
      "{'loss': 2.217, 'learning_rate': 0.00019997972289848503, 'epoch': 0.07}         \n",
      "{'loss': 2.0667, 'learning_rate': 0.00019997351589651408, 'epoch': 0.07}        \n",
      "{'loss': 1.8724, 'learning_rate': 0.0001999664814495453, 'epoch': 0.08}         \n",
      "{'loss': 1.9841, 'learning_rate': 0.00019995861961580024, 'epoch': 0.08}        \n",
      "{'loss': 2.0903, 'learning_rate': 0.00019994993046034846, 'epoch': 0.08}        \n",
      "{'loss': 1.9658, 'learning_rate': 0.00019994041405510705, 'epoch': 0.08}        \n",
      "{'loss': 2.1317, 'learning_rate': 0.00019993007047883988, 'epoch': 0.08}        \n",
      "{'loss': 1.9163, 'learning_rate': 0.00019991889981715698, 'epoch': 0.09}        \n",
      "{'loss': 2.2688, 'learning_rate': 0.00019990690216251396, 'epoch': 0.09}        \n",
      "{'loss': 1.8167, 'learning_rate': 0.00019989407761421109, 'epoch': 0.09}        \n",
      "{'loss': 1.8495, 'learning_rate': 0.00019988042627839257, 'epoch': 0.09}        \n",
      "{'loss': 1.6772, 'learning_rate': 0.0001998659482680456, 'epoch': 0.09}         \n",
      "{'loss': 1.8672, 'learning_rate': 0.00019985064370299957, 'epoch': 0.09}        \n",
      "{'loss': 1.8491, 'learning_rate': 0.00019983451270992478, 'epoch': 0.1}         \n",
      "{'loss': 1.6903, 'learning_rate': 0.00019981755542233177, 'epoch': 0.1}         \n",
      "{'loss': 2.1412, 'learning_rate': 0.00019979977198056984, 'epoch': 0.1}         \n",
      "{'loss': 1.9412, 'learning_rate': 0.00019978116253182627, 'epoch': 0.1}         \n",
      "{'loss': 2.1219, 'learning_rate': 0.0001997617272301248, 'epoch': 0.1}          \n",
      "{'loss': 1.6224, 'learning_rate': 0.00019974146623632434, 'epoch': 0.1}         \n",
      "{'loss': 1.6783, 'learning_rate': 0.00019972037971811802, 'epoch': 0.11}        \n",
      "{'loss': 1.7759, 'learning_rate': 0.00019969846785003134, 'epoch': 0.11}        \n",
      "{'loss': 1.8741, 'learning_rate': 0.00019967573081342103, 'epoch': 0.11}        \n",
      "{'loss': 1.5906, 'learning_rate': 0.00019965216879647333, 'epoch': 0.11}        \n",
      "{'loss': 1.542, 'learning_rate': 0.00019962778199420265, 'epoch': 0.11}         \n",
      "{'loss': 1.5305, 'learning_rate': 0.00019960257060844984, 'epoch': 0.12}        \n",
      "{'loss': 2.014, 'learning_rate': 0.00019957653484788053, 'epoch': 0.12}         \n",
      "{'loss': 1.7139, 'learning_rate': 0.00019954967492798333, 'epoch': 0.12}        \n",
      "{'loss': 1.9263, 'learning_rate': 0.0001995219910710682, 'epoch': 0.12}         \n",
      "{'loss': 1.7176, 'learning_rate': 0.00019949348350626456, 'epoch': 0.12}        \n",
      "{'loss': 1.6088, 'learning_rate': 0.0001994641524695193, 'epoch': 0.12}         \n",
      "{'loss': 1.6171, 'learning_rate': 0.00019943399820359484, 'epoch': 0.13}        \n",
      "{'loss': 1.6125, 'learning_rate': 0.0001994030209580673, 'epoch': 0.13}         \n",
      "{'loss': 1.6191, 'learning_rate': 0.00019937122098932428, 'epoch': 0.13}        \n",
      "{'loss': 1.5649, 'learning_rate': 0.00019933859856056265, 'epoch': 0.13}        \n",
      "{'loss': 1.6125, 'learning_rate': 0.0001993051539417867, 'epoch': 0.13}         \n",
      "{'loss': 1.8238, 'learning_rate': 0.0001992708874098054, 'epoch': 0.13}         \n",
      "{'loss': 1.6852, 'learning_rate': 0.0001992357992482307, 'epoch': 0.14}         \n",
      "{'loss': 1.9728, 'learning_rate': 0.00019919988974747473, 'epoch': 0.14}        \n",
      "{'loss': 1.5795, 'learning_rate': 0.0001991631592047475, 'epoch': 0.14}         \n",
      "{'loss': 1.8055, 'learning_rate': 0.00019912560792405463, 'epoch': 0.14}        \n",
      "{'loss': 1.3393, 'learning_rate': 0.00019908723621619464, 'epoch': 0.14}        \n",
      "{'loss': 1.7657, 'learning_rate': 0.00019904804439875633, 'epoch': 0.15}        \n",
      "{'loss': 1.7433, 'learning_rate': 0.0001990080327961164, 'epoch': 0.15}         \n",
      "{'loss': 1.7194, 'learning_rate': 0.00019896720173943658, 'epoch': 0.15}        \n",
      "{'loss': 1.7363, 'learning_rate': 0.00019892555156666089, 'epoch': 0.15}        \n",
      "{'loss': 1.7777, 'learning_rate': 0.00019888308262251285, 'epoch': 0.15}        \n",
      "{'loss': 1.5714, 'learning_rate': 0.0001988397952584928, 'epoch': 0.15}         \n",
      "{'loss': 1.5078, 'learning_rate': 0.00019879568983287467, 'epoch': 0.16}        \n",
      "{'loss': 1.5367, 'learning_rate': 0.0001987507667107033, 'epoch': 0.16}         \n",
      "{'loss': 1.748, 'learning_rate': 0.00019870502626379127, 'epoch': 0.16}         \n",
      "{'loss': 1.6074, 'learning_rate': 0.00019865846887071596, 'epoch': 0.16}        \n",
      "{'loss': 1.5185, 'learning_rate': 0.00019861109491681617, 'epoch': 0.16}        \n",
      "{'loss': 1.6715, 'learning_rate': 0.00019856290479418917, 'epoch': 0.17}        \n",
      "{'loss': 1.7319, 'learning_rate': 0.0001985138989016874, 'epoch': 0.17}         \n",
      "{'loss': 1.8106, 'learning_rate': 0.00019846407764491502, 'epoch': 0.17}        \n",
      "{'loss': 1.7298, 'learning_rate': 0.00019841344143622474, 'epoch': 0.17}        \n",
      "{'loss': 1.6784, 'learning_rate': 0.00019836199069471437, 'epoch': 0.17}        \n",
      "{'loss': 1.551, 'learning_rate': 0.00019830972584622324, 'epoch': 0.17}         \n",
      "{'loss': 1.4555, 'learning_rate': 0.00019825664732332884, 'epoch': 0.18}        \n",
      "{'loss': 1.328, 'learning_rate': 0.00019820275556534304, 'epoch': 0.18}         \n",
      "{'loss': 1.6593, 'learning_rate': 0.00019814805101830865, 'epoch': 0.18}        \n",
      "{'loss': 1.5789, 'learning_rate': 0.00019809253413499565, 'epoch': 0.18}        \n",
      "{'loss': 1.4578, 'learning_rate': 0.00019803620537489736, 'epoch': 0.18}        \n",
      "{'loss': 1.2773, 'learning_rate': 0.00019797906520422677, 'epoch': 0.18}        \n",
      "{'loss': 1.2953, 'learning_rate': 0.00019792111409591264, 'epoch': 0.19}        \n",
      "{'loss': 1.4358, 'learning_rate': 0.00019786235252959553, 'epoch': 0.19}        \n",
      "{'loss': 1.3789, 'learning_rate': 0.0001978027809916239, 'epoch': 0.19}         \n",
      "{'loss': 1.4079, 'learning_rate': 0.00019774239997505, 'epoch': 0.19}           \n",
      "{'loss': 1.3796, 'learning_rate': 0.00019768120997962592, 'epoch': 0.19}        \n",
      "{'loss': 1.6552, 'learning_rate': 0.00019761921151179937, 'epoch': 0.2}         \n",
      "{'loss': 1.5152, 'learning_rate': 0.00019755640508470942, 'epoch': 0.2}         \n",
      "{'loss': 1.4422, 'learning_rate': 0.00019749279121818235, 'epoch': 0.2}         \n",
      "{'loss': 1.646, 'learning_rate': 0.00019742837043872747, 'epoch': 0.2}          \n",
      "{'loss': 1.3238, 'learning_rate': 0.00019736314327953243, 'epoch': 0.2}         \n",
      "{'loss': 1.231, 'learning_rate': 0.00019729711028045909, 'epoch': 0.2}          \n",
      "{'loss': 1.373, 'learning_rate': 0.00019723027198803895, 'epoch': 0.21}         \n",
      "{'loss': 1.5402, 'learning_rate': 0.00019716262895546857, 'epoch': 0.21}        \n",
      "{'loss': 1.6413, 'learning_rate': 0.0001970941817426052, 'epoch': 0.21}         \n",
      "{'loss': 1.5122, 'learning_rate': 0.00019702493091596187, 'epoch': 0.21}        \n",
      "{'loss': 1.7858, 'learning_rate': 0.0001969548770487029, 'epoch': 0.21}         \n",
      "{'loss': 1.3895, 'learning_rate': 0.00019688402072063903, 'epoch': 0.21}        \n",
      "{'loss': 1.2827, 'learning_rate': 0.00019681236251822273, 'epoch': 0.22}        \n",
      "{'loss': 1.4903, 'learning_rate': 0.00019673990303454333, 'epoch': 0.22}        \n",
      "{'loss': 1.2672, 'learning_rate': 0.00019666664286932198, 'epoch': 0.22}        \n",
      "{'loss': 1.6648, 'learning_rate': 0.00019659258262890683, 'epoch': 0.22}        \n",
      "{'loss': 1.5315, 'learning_rate': 0.00019651772292626803, 'epoch': 0.22}        \n",
      "{'loss': 1.4611, 'learning_rate': 0.0001964420643809925, 'epoch': 0.23}         \n",
      "{'loss': 1.3614, 'learning_rate': 0.00019636560761927897, 'epoch': 0.23}        \n",
      "{'loss': 1.2625, 'learning_rate': 0.00019628835327393262, 'epoch': 0.23}        \n",
      "{'loss': 1.6225, 'learning_rate': 0.00019621030198436006, 'epoch': 0.23}        \n",
      "{'loss': 1.2941, 'learning_rate': 0.00019613145439656386, 'epoch': 0.23}        \n",
      "{'loss': 1.2746, 'learning_rate': 0.00019605181116313724, 'epoch': 0.23}        \n",
      "{'loss': 1.339, 'learning_rate': 0.00019597137294325877, 'epoch': 0.24}         \n",
      "{'loss': 1.4639, 'learning_rate': 0.00019589014040268676, 'epoch': 0.24}        \n",
      "{'loss': 1.117, 'learning_rate': 0.0001958081142137539, 'epoch': 0.24}          \n",
      "{'loss': 1.4489, 'learning_rate': 0.0001957252950553616, 'epoch': 0.24}         \n",
      "{'loss': 1.2704, 'learning_rate': 0.0001956416836129743, 'epoch': 0.24}         \n",
      "{'loss': 1.4774, 'learning_rate': 0.0001955572805786141, 'epoch': 0.25}         \n",
      "{'loss': 1.297, 'learning_rate': 0.00019547208665085457, 'epoch': 0.25}         \n",
      "{'loss': 1.6642, 'learning_rate': 0.0001953861025348154, 'epoch': 0.25}         \n",
      "{'loss': 1.2772, 'learning_rate': 0.00019529932894215638, 'epoch': 0.25}        \n",
      "{'loss': 1.3209, 'learning_rate': 0.00019521176659107142, 'epoch': 0.25}        \n",
      "{'loss': 1.1517, 'learning_rate': 0.00019512341620628277, 'epoch': 0.25}        \n",
      "{'loss': 1.2906, 'learning_rate': 0.0001950342785190349, 'epoch': 0.26}         \n",
      "{'loss': 1.2866, 'learning_rate': 0.00019494435426708855, 'epoch': 0.26}        \n",
      "{'loss': 1.6365, 'learning_rate': 0.00019485364419471454, 'epoch': 0.26}        \n",
      "{'loss': 1.2616, 'learning_rate': 0.00019476214905268772, 'epoch': 0.26}        \n",
      "{'loss': 1.5916, 'learning_rate': 0.0001946698695982806, 'epoch': 0.26}         \n",
      "{'loss': 1.2254, 'learning_rate': 0.00019457680659525715, 'epoch': 0.26}        \n",
      "{'loss': 1.6915, 'learning_rate': 0.00019448296081386656, 'epoch': 0.27}        \n",
      "{'loss': 1.4095, 'learning_rate': 0.00019438833303083678, 'epoch': 0.27}        \n",
      "{'loss': 1.5215, 'learning_rate': 0.00019429292402936804, 'epoch': 0.27}        \n",
      "{'loss': 1.3933, 'learning_rate': 0.0001941967345991265, 'epoch': 0.27}         \n",
      "{'loss': 1.5547, 'learning_rate': 0.00019409976553623766, 'epoch': 0.27}        \n",
      "{'loss': 1.1514, 'learning_rate': 0.0001940020176432797, 'epoch': 0.28}         \n",
      "{'loss': 1.1569, 'learning_rate': 0.00019390349172927697, 'epoch': 0.28}        \n",
      "{'loss': 1.3406, 'learning_rate': 0.00019380418860969322, 'epoch': 0.28}        \n",
      "{'loss': 1.4468, 'learning_rate': 0.00019370410910642471, 'epoch': 0.28}        \n",
      "{'loss': 1.2899, 'learning_rate': 0.00019360325404779374, 'epoch': 0.28}        \n",
      "{'loss': 1.4721, 'learning_rate': 0.0001935016242685415, 'epoch': 0.28}         \n",
      "{'loss': 1.4829, 'learning_rate': 0.0001933992206098213, 'epoch': 0.29}         \n",
      "{'loss': 1.2173, 'learning_rate': 0.0001932960439191915, 'epoch': 0.29}         \n",
      "{'loss': 1.0569, 'learning_rate': 0.0001931920950506087, 'epoch': 0.29}         \n",
      "{'loss': 1.4345, 'learning_rate': 0.00019308737486442045, 'epoch': 0.29}        \n",
      "{'loss': 1.5504, 'learning_rate': 0.00019298188422735822, 'epoch': 0.29}        \n",
      "{'loss': 1.4416, 'learning_rate': 0.00019287562401253022, 'epoch': 0.29}        \n",
      "{'loss': 1.4588, 'learning_rate': 0.0001927685950994143, 'epoch': 0.3}          \n",
      "{'loss': 1.2637, 'learning_rate': 0.0001926607983738504, 'epoch': 0.3}          \n",
      "{'loss': 1.9078, 'learning_rate': 0.00019255223472803334, 'epoch': 0.3}         \n",
      "{'loss': 1.5286, 'learning_rate': 0.00019244290506050568, 'epoch': 0.3}         \n",
      "{'loss': 1.2521, 'learning_rate': 0.00019233281027614986, 'epoch': 0.3}         \n",
      "{'loss': 1.4519, 'learning_rate': 0.00019222195128618106, 'epoch': 0.31}        \n",
      "{'loss': 1.5602, 'learning_rate': 0.00019211032900813934, 'epoch': 0.31}        \n",
      "{'loss': 1.3294, 'learning_rate': 0.00019199794436588243, 'epoch': 0.31}        \n",
      "{'loss': 1.6378, 'learning_rate': 0.00019188479828957772, 'epoch': 0.31}        \n",
      "{'loss': 1.1816, 'learning_rate': 0.00019177089171569482, 'epoch': 0.31}        \n",
      "{'loss': 1.7883, 'learning_rate': 0.00019165622558699763, 'epoch': 0.31}        \n",
      "{'loss': 1.3947, 'learning_rate': 0.00019154080085253666, 'epoch': 0.32}        \n",
      "{'loss': 1.1847, 'learning_rate': 0.00019142461846764105, 'epoch': 0.32}        \n",
      "{'loss': 0.9716, 'learning_rate': 0.0001913076793939109, 'epoch': 0.32}         \n",
      "{'loss': 1.443, 'learning_rate': 0.00019118998459920902, 'epoch': 0.32}         \n",
      "{'loss': 1.2568, 'learning_rate': 0.00019107153505765306, 'epoch': 0.32}        \n",
      "{'loss': 1.2343, 'learning_rate': 0.0001909523317496075, 'epoch': 0.33}         \n",
      "{'loss': 1.2636, 'learning_rate': 0.0001908323756616754, 'epoch': 0.33}         \n",
      "{'loss': 1.6315, 'learning_rate': 0.0001907116677866904, 'epoch': 0.33}         \n",
      "{'loss': 1.5739, 'learning_rate': 0.00019059020912370834, 'epoch': 0.33}        \n",
      "{'loss': 1.4554, 'learning_rate': 0.0001904680006779991, 'epoch': 0.33}         \n",
      "{'loss': 1.3734, 'learning_rate': 0.00019034504346103823, 'epoch': 0.33}        \n",
      "{'loss': 1.0297, 'learning_rate': 0.0001902213384904986, 'epoch': 0.34}         \n",
      "{'loss': 1.1952, 'learning_rate': 0.0001900968867902419, 'epoch': 0.34}         \n",
      "{'loss': 1.2525, 'learning_rate': 0.0001899716893903104, 'epoch': 0.34}         \n",
      "{'loss': 1.5596, 'learning_rate': 0.00018984574732691808, 'epoch': 0.34}        \n",
      "{'loss': 1.191, 'learning_rate': 0.00018971906164244232, 'epoch': 0.34}         \n",
      "{'loss': 1.2082, 'learning_rate': 0.00018959163338541518, 'epoch': 0.34}        \n",
      "{'loss': 1.2451, 'learning_rate': 0.00018946346361051474, 'epoch': 0.35}        \n",
      "{'loss': 1.328, 'learning_rate': 0.00018933455337855632, 'epoch': 0.35}         \n",
      "{'loss': 1.137, 'learning_rate': 0.00018920490375648377, 'epoch': 0.35}         \n",
      "{'loss': 1.166, 'learning_rate': 0.00018907451581736054, 'epoch': 0.35}         \n",
      "{'loss': 1.1798, 'learning_rate': 0.000188943390640361, 'epoch': 0.35}          \n",
      "{'loss': 1.2233, 'learning_rate': 0.00018881152931076126, 'epoch': 0.36}        \n",
      "{'loss': 1.5745, 'learning_rate': 0.00018867893291993028, 'epoch': 0.36}        \n",
      "{'loss': 1.1885, 'learning_rate': 0.000188545602565321, 'epoch': 0.36}          \n",
      "{'loss': 1.2736, 'learning_rate': 0.00018841153935046098, 'epoch': 0.36}        \n",
      "{'loss': 1.2264, 'learning_rate': 0.0001882767443849435, 'epoch': 0.36}         \n",
      "{'loss': 1.0659, 'learning_rate': 0.00018814121878441814, 'epoch': 0.36}        \n",
      "{'loss': 1.5975, 'learning_rate': 0.00018800496367058186, 'epoch': 0.37}        \n",
      "{'loss': 1.4609, 'learning_rate': 0.00018786798017116942, 'epoch': 0.37}        \n",
      "{'loss': 1.1005, 'learning_rate': 0.0001877302694199442, 'epoch': 0.37}         \n",
      "{'loss': 1.1236, 'learning_rate': 0.0001875918325566888, 'epoch': 0.37}         \n",
      "{'loss': 1.2346, 'learning_rate': 0.00018745267072719555, 'epoch': 0.37}        \n",
      "{'loss': 1.2084, 'learning_rate': 0.00018731278508325708, 'epoch': 0.37}        \n",
      "{'loss': 1.2357, 'learning_rate': 0.0001871721767826568, 'epoch': 0.38}         \n",
      "{'loss': 1.3833, 'learning_rate': 0.00018703084698915934, 'epoch': 0.38}        \n",
      "{'loss': 1.0646, 'learning_rate': 0.00018688879687250067, 'epoch': 0.38}        \n",
      "{'loss': 1.193, 'learning_rate': 0.00018674602760837883, 'epoch': 0.38}         \n",
      "{'loss': 1.4101, 'learning_rate': 0.00018660254037844388, 'epoch': 0.38}        \n",
      "{'loss': 1.2306, 'learning_rate': 0.00018645833637028825, 'epoch': 0.39}        \n",
      "{'loss': 1.238, 'learning_rate': 0.0001863134167774369, 'epoch': 0.39}          \n",
      "{'loss': 1.0317, 'learning_rate': 0.00018616778279933742, 'epoch': 0.39}        \n",
      "{'loss': 1.4579, 'learning_rate': 0.0001860214356413501, 'epoch': 0.39}         \n",
      "{'loss': 1.2764, 'learning_rate': 0.00018587437651473794, 'epoch': 0.39}        \n",
      "{'loss': 1.0207, 'learning_rate': 0.0001857266066366567, 'epoch': 0.39}         \n",
      "{'loss': 1.2848, 'learning_rate': 0.00018557812723014476, 'epoch': 0.4}         \n",
      "{'loss': 0.9649, 'learning_rate': 0.000185428939524113, 'epoch': 0.4}           \n",
      "{'loss': 1.404, 'learning_rate': 0.0001852790447533347, 'epoch': 0.4}           \n",
      "{'loss': 1.2235, 'learning_rate': 0.00018512844415843514, 'epoch': 0.4}         \n",
      "{'loss': 1.2797, 'learning_rate': 0.00018497713898588155, 'epoch': 0.4}         \n",
      "{'loss': 1.0593, 'learning_rate': 0.00018482513048797267, 'epoch': 0.4}         \n",
      "{'loss': 1.3277, 'learning_rate': 0.00018467241992282843, 'epoch': 0.41}        \n",
      "{'loss': 1.1141, 'learning_rate': 0.0001845190085543795, 'epoch': 0.41}         \n",
      "{'loss': 1.134, 'learning_rate': 0.00018436489765235684, 'epoch': 0.41}         \n",
      "{'loss': 1.1736, 'learning_rate': 0.00018421008849228118, 'epoch': 0.41}        \n",
      "{'loss': 1.1026, 'learning_rate': 0.00018405458235545255, 'epoch': 0.41}        \n",
      "{'loss': 1.363, 'learning_rate': 0.0001838983805289396, 'epoch': 0.42}          \n",
      "{'loss': 1.1567, 'learning_rate': 0.0001837414843055689, 'epoch': 0.42}         \n",
      "{'loss': 0.9876, 'learning_rate': 0.00018358389498391426, 'epoch': 0.42}        \n",
      "{'loss': 1.0202, 'learning_rate': 0.00018342561386828615, 'epoch': 0.42}        \n",
      "{'loss': 1.1781, 'learning_rate': 0.00018326664226872065, 'epoch': 0.42}        \n",
      "{'loss': 1.022, 'learning_rate': 0.0001831069815009688, 'epoch': 0.42}          \n",
      "{'loss': 1.2906, 'learning_rate': 0.0001829466328864856, 'epoch': 0.43}         \n",
      "{'loss': 1.0442, 'learning_rate': 0.0001827855977524191, 'epoch': 0.43}         \n",
      "{'loss': 0.978, 'learning_rate': 0.0001826238774315995, 'epoch': 0.43}          \n",
      "{'loss': 1.3706, 'learning_rate': 0.00018246147326252796, 'epoch': 0.43}        \n",
      "{'loss': 1.1101, 'learning_rate': 0.00018229838658936564, 'epoch': 0.43}        \n",
      "{'loss': 1.2797, 'learning_rate': 0.00018213461876192256, 'epoch': 0.44}        \n",
      "{'loss': 1.2946, 'learning_rate': 0.0001819701711356464, 'epoch': 0.44}         \n",
      "{'loss': 1.0095, 'learning_rate': 0.0001818050450716113, 'epoch': 0.44}         \n",
      "{'loss': 1.3349, 'learning_rate': 0.0001816392419365065, 'epoch': 0.44}         \n",
      "{'loss': 1.4658, 'learning_rate': 0.00018147276310262527, 'epoch': 0.44}        \n",
      "{'loss': 1.0531, 'learning_rate': 0.00018130560994785325, 'epoch': 0.44}        \n",
      "{'loss': 1.414, 'learning_rate': 0.00018113778385565733, 'epoch': 0.45}         \n",
      "{'loss': 1.2449, 'learning_rate': 0.0001809692862150739, 'epoch': 0.45}         \n",
      "{'loss': 1.1455, 'learning_rate': 0.00018080011842069765, 'epoch': 0.45}        \n",
      "{'loss': 1.2566, 'learning_rate': 0.00018063028187266986, 'epoch': 0.45}        \n",
      "{'loss': 1.0509, 'learning_rate': 0.00018045977797666684, 'epoch': 0.45}        \n",
      "{'loss': 1.069, 'learning_rate': 0.00018028860814388827, 'epoch': 0.45}         \n",
      "{'loss': 1.2071, 'learning_rate': 0.00018011677379104556, 'epoch': 0.46}        \n",
      "{'loss': 1.2902, 'learning_rate': 0.00017994427634035015, 'epoch': 0.46}        \n",
      "{'loss': 1.1563, 'learning_rate': 0.00017977111721950164, 'epoch': 0.46}        \n",
      "{'loss': 1.1976, 'learning_rate': 0.00017959729786167614, 'epoch': 0.46}        \n",
      "{'loss': 1.4531, 'learning_rate': 0.0001794228197055142, 'epoch': 0.46}         \n",
      "{'loss': 1.1767, 'learning_rate': 0.00017924768419510904, 'epoch': 0.47}        \n",
      "{'loss': 0.9682, 'learning_rate': 0.00017907189277999462, 'epoch': 0.47}        \n",
      "{'loss': 1.1541, 'learning_rate': 0.00017889544691513355, 'epoch': 0.47}        \n",
      "{'loss': 1.1907, 'learning_rate': 0.00017871834806090501, 'epoch': 0.47}        \n",
      "{'loss': 1.1105, 'learning_rate': 0.0001785405976830929, 'epoch': 0.47}         \n",
      "{'loss': 1.2978, 'learning_rate': 0.00017836219725287338, 'epoch': 0.47}        \n",
      "{'loss': 1.0065, 'learning_rate': 0.000178183148246803, 'epoch': 0.48}          \n",
      "{'loss': 1.1052, 'learning_rate': 0.00017800345214680623, 'epoch': 0.48}        \n",
      "{'loss': 1.1724, 'learning_rate': 0.00017782311044016338, 'epoch': 0.48}        \n",
      "{'loss': 1.5674, 'learning_rate': 0.0001776421246194982, 'epoch': 0.48}         \n",
      "{'loss': 0.9435, 'learning_rate': 0.00017746049618276545, 'epoch': 0.48}        \n",
      "{'loss': 1.082, 'learning_rate': 0.00017727822663323882, 'epoch': 0.48}         \n",
      "{'loss': 1.2586, 'learning_rate': 0.00017709531747949796, 'epoch': 0.49}        \n",
      "{'loss': 1.2675, 'learning_rate': 0.00017691177023541657, 'epoch': 0.49}        \n",
      "{'loss': 0.9462, 'learning_rate': 0.0001767275864201494, 'epoch': 0.49}         \n",
      "{'loss': 1.2039, 'learning_rate': 0.00017654276755811997, 'epoch': 0.49}        \n",
      "{'loss': 1.1997, 'learning_rate': 0.00017635731517900782, 'epoch': 0.49}        \n",
      "{'loss': 0.9902, 'learning_rate': 0.00017617123081773591, 'epoch': 0.5}         \n",
      "{'loss': 1.0151, 'learning_rate': 0.0001759845160144579, 'epoch': 0.5}          \n",
      "{'loss': 0.9536, 'learning_rate': 0.0001757971723145453, 'epoch': 0.5}          \n",
      "{'loss': 1.2809, 'learning_rate': 0.0001756092012685749, 'epoch': 0.5}          \n",
      "{'loss': 1.0678, 'learning_rate': 0.00017542060443231572, 'epoch': 0.5}         \n",
      "{'loss': 1.1978, 'learning_rate': 0.00017523138336671628, 'epoch': 0.5}         \n",
      "{'loss': 1.0537, 'learning_rate': 0.00017504153963789155, 'epoch': 0.51}        \n",
      "{'loss': 1.1053, 'learning_rate': 0.00017485107481711012, 'epoch': 0.51}        \n",
      "{'loss': 1.2309, 'learning_rate': 0.00017465999048078113, 'epoch': 0.51}        \n",
      "{'loss': 1.4917, 'learning_rate': 0.00017446828821044124, 'epoch': 0.51}        \n",
      "{'loss': 1.074, 'learning_rate': 0.00017427596959274143, 'epoch': 0.51}         \n",
      "{'loss': 1.1281, 'learning_rate': 0.00017408303621943417, 'epoch': 0.52}        \n",
      "{'loss': 1.2564, 'learning_rate': 0.0001738894896873598, 'epoch': 0.52}         \n",
      "{'loss': 1.3533, 'learning_rate': 0.00017369533159843369, 'epoch': 0.52}        \n",
      "{'loss': 0.9177, 'learning_rate': 0.00017350056355963287, 'epoch': 0.52}        \n",
      "{'loss': 1.3106, 'learning_rate': 0.00017330518718298264, 'epoch': 0.52}        \n",
      "{'loss': 1.2189, 'learning_rate': 0.00017310920408554332, 'epoch': 0.52}        \n",
      "{'loss': 0.9046, 'learning_rate': 0.00017291261588939686, 'epoch': 0.53}        \n",
      "{'loss': 1.1437, 'learning_rate': 0.00017271542422163337, 'epoch': 0.53}        \n",
      "{'loss': 1.2989, 'learning_rate': 0.00017251763071433765, 'epoch': 0.53}        \n",
      "{'loss': 1.3818, 'learning_rate': 0.00017231923700457578, 'epoch': 0.53}        \n",
      "{'loss': 1.0027, 'learning_rate': 0.00017212024473438147, 'epoch': 0.53}        \n",
      "{'loss': 0.8564, 'learning_rate': 0.00017192065555074245, 'epoch': 0.53}        \n",
      "{'loss': 0.984, 'learning_rate': 0.000171720471105587, 'epoch': 0.54}           \n",
      "{'loss': 1.1449, 'learning_rate': 0.00017151969305577005, 'epoch': 0.54}        \n",
      "{'loss': 0.9327, 'learning_rate': 0.00017131832306305965, 'epoch': 0.54}        \n",
      "{'loss': 0.9282, 'learning_rate': 0.0001711163627941231, 'epoch': 0.54}         \n",
      "{'loss': 1.0359, 'learning_rate': 0.0001709138139205133, 'epoch': 0.54}         \n",
      "{'loss': 1.0436, 'learning_rate': 0.00017071067811865476, 'epoch': 0.55}        \n",
      "{'loss': 1.3197, 'learning_rate': 0.00017050695706982976, 'epoch': 0.55}        \n",
      "{'loss': 1.1826, 'learning_rate': 0.00017030265246016443, 'epoch': 0.55}        \n",
      "{'loss': 1.2263, 'learning_rate': 0.00017009776598061495, 'epoch': 0.55}        \n",
      "{'loss': 0.944, 'learning_rate': 0.00016989229932695335, 'epoch': 0.55}         \n",
      "{'loss': 1.0701, 'learning_rate': 0.00016968625419975353, 'epoch': 0.55}        \n",
      "{'loss': 1.1761, 'learning_rate': 0.00016947963230437725, 'epoch': 0.56}        \n",
      "{'loss': 1.078, 'learning_rate': 0.00016927243535095997, 'epoch': 0.56}         \n",
      "{'loss': 1.1141, 'learning_rate': 0.00016906466505439666, 'epoch': 0.56}        \n",
      "{'loss': 1.0163, 'learning_rate': 0.0001688563231343277, 'epoch': 0.56}         \n",
      "{'loss': 1.1127, 'learning_rate': 0.0001686474113151246, 'epoch': 0.56}         \n",
      "{'loss': 0.8477, 'learning_rate': 0.00016843793132587567, 'epoch': 0.56}        \n",
      "{'loss': 1.0424, 'learning_rate': 0.00016822788490037177, 'epoch': 0.57}        \n",
      "{'loss': 1.1499, 'learning_rate': 0.00016801727377709194, 'epoch': 0.57}        \n",
      "{'loss': 1.1249, 'learning_rate': 0.0001678060996991891, 'epoch': 0.57}         \n",
      "{'loss': 1.0213, 'learning_rate': 0.00016759436441447545, 'epoch': 0.57}        \n",
      "{'loss': 1.0985, 'learning_rate': 0.00016738206967540812, 'epoch': 0.57}        \n",
      "{'loss': 1.3938, 'learning_rate': 0.0001671692172390747, 'epoch': 0.58}         \n",
      "{'loss': 1.1506, 'learning_rate': 0.00016695580886717858, 'epoch': 0.58}        \n",
      "{'loss': 1.1804, 'learning_rate': 0.00016674184632602446, 'epoch': 0.58}        \n",
      "{'loss': 1.0409, 'learning_rate': 0.00016652733138650366, 'epoch': 0.58}        \n",
      "{'loss': 1.1112, 'learning_rate': 0.00016631226582407952, 'epoch': 0.58}        \n",
      "{'loss': 0.8816, 'learning_rate': 0.00016609665141877278, 'epoch': 0.58}        \n",
      "{'loss': 1.445, 'learning_rate': 0.00016588048995514658, 'epoch': 0.59}         \n",
      "{'loss': 1.3758, 'learning_rate': 0.00016566378322229204, 'epoch': 0.59}        \n",
      "{'loss': 1.7078, 'learning_rate': 0.00016544653301381316, 'epoch': 0.59}        \n",
      "{'loss': 1.0452, 'learning_rate': 0.00016522874112781213, 'epoch': 0.59}        \n",
      "{'loss': 1.3393, 'learning_rate': 0.00016501040936687443, 'epoch': 0.59}        \n",
      "{'loss': 1.2217, 'learning_rate': 0.0001647915395380539, 'epoch': 0.6}          \n",
      "{'loss': 1.324, 'learning_rate': 0.0001645721334528577, 'epoch': 0.6}           \n",
      "{'loss': 1.0058, 'learning_rate': 0.00016435219292723147, 'epoch': 0.6}         \n",
      "{'loss': 1.2333, 'learning_rate': 0.0001641317197815442, 'epoch': 0.6}          \n",
      "{'loss': 0.9366, 'learning_rate': 0.0001639107158405732, 'epoch': 0.6}          \n",
      "{'loss': 0.9716, 'learning_rate': 0.00016368918293348892, 'epoch': 0.6}         \n",
      "{'loss': 1.3653, 'learning_rate': 0.00016346712289383997, 'epoch': 0.61}        \n",
      "{'loss': 0.9573, 'learning_rate': 0.00016324453755953773, 'epoch': 0.61}        \n",
      "{'loss': 0.9213, 'learning_rate': 0.00016302142877284138, 'epoch': 0.61}        \n",
      "{'loss': 1.3983, 'learning_rate': 0.00016279779838034242, 'epoch': 0.61}        \n",
      "{'loss': 1.0354, 'learning_rate': 0.00016257364823294958, 'epoch': 0.61}        \n",
      "{'loss': 1.0357, 'learning_rate': 0.00016234898018587337, 'epoch': 0.61}        \n",
      "{'loss': 1.0255, 'learning_rate': 0.00016212379609861077, 'epoch': 0.62}        \n",
      "{'loss': 0.9839, 'learning_rate': 0.00016189809783492983, 'epoch': 0.62}        \n",
      "{'loss': 1.1813, 'learning_rate': 0.00016167188726285434, 'epoch': 0.62}        \n",
      "{'loss': 0.9074, 'learning_rate': 0.00016144516625464812, 'epoch': 0.62}        \n",
      "{'loss': 1.0489, 'learning_rate': 0.00016121793668679986, 'epoch': 0.62}        \n",
      "{'loss': 1.3421, 'learning_rate': 0.00016099020044000727, 'epoch': 0.63}        \n",
      "{'loss': 0.8739, 'learning_rate': 0.00016076195939916183, 'epoch': 0.63}        \n",
      "{'loss': 0.8978, 'learning_rate': 0.00016053321545333283, 'epoch': 0.63}        \n",
      "{'loss': 1.1336, 'learning_rate': 0.00016030397049575203, 'epoch': 0.63}        \n",
      "{'loss': 1.3453, 'learning_rate': 0.0001600742264237979, 'epoch': 0.63}         \n",
      "{'loss': 1.6805, 'learning_rate': 0.00015984398513897984, 'epoch': 0.63}        \n",
      "{'loss': 1.1131, 'learning_rate': 0.00015961324854692254, 'epoch': 0.64}        \n",
      "{'loss': 0.883, 'learning_rate': 0.00015938201855735014, 'epoch': 0.64}         \n",
      "{'loss': 1.0674, 'learning_rate': 0.00015915029708407055, 'epoch': 0.64}        \n",
      "{'loss': 0.6729, 'learning_rate': 0.00015891808604495938, 'epoch': 0.64}        \n",
      "{'loss': 1.281, 'learning_rate': 0.00015868538736194427, 'epoch': 0.64}         \n",
      "{'loss': 1.0797, 'learning_rate': 0.0001584522029609889, 'epoch': 0.64}         \n",
      "{'loss': 0.9395, 'learning_rate': 0.00015821853477207708, 'epoch': 0.65}        \n",
      "{'loss': 1.0198, 'learning_rate': 0.00015798438472919675, 'epoch': 0.65}        \n",
      "{'loss': 1.1557, 'learning_rate': 0.000157749754770324, 'epoch': 0.65}          \n",
      "{'loss': 1.0479, 'learning_rate': 0.00015751464683740697, 'epoch': 0.65}        \n",
      "{'loss': 0.8966, 'learning_rate': 0.00015727906287634985, 'epoch': 0.65}        \n",
      "{'loss': 1.1105, 'learning_rate': 0.00015704300483699676, 'epoch': 0.66}        \n",
      "{'loss': 0.9875, 'learning_rate': 0.00015680647467311557, 'epoch': 0.66}        \n",
      "{'loss': 0.9923, 'learning_rate': 0.00015656947434238176, 'epoch': 0.66}        \n",
      "{'loss': 1.2798, 'learning_rate': 0.0001563320058063622, 'epoch': 0.66}         \n",
      "{'loss': 0.9432, 'learning_rate': 0.00015609407103049896, 'epoch': 0.66}        \n",
      "{'loss': 1.4179, 'learning_rate': 0.00015585567198409298, 'epoch': 0.66}        \n",
      "{'loss': 1.3454, 'learning_rate': 0.00015561681064028783, 'epoch': 0.67}        \n",
      "{'loss': 1.1245, 'learning_rate': 0.0001553774889760533, 'epoch': 0.67}         \n",
      "{'loss': 0.9363, 'learning_rate': 0.00015513770897216918, 'epoch': 0.67}        \n",
      "{'loss': 1.0768, 'learning_rate': 0.00015489747261320866, 'epoch': 0.67}        \n",
      "{'loss': 1.3689, 'learning_rate': 0.0001546567818875221, 'epoch': 0.67}         \n",
      "{'loss': 0.8812, 'learning_rate': 0.00015441563878722042, 'epoch': 0.67}        \n",
      "{'loss': 1.0214, 'learning_rate': 0.00015417404530815881, 'epoch': 0.68}        \n",
      "{'loss': 0.945, 'learning_rate': 0.00015393200344991995, 'epoch': 0.68}         \n",
      "{'loss': 0.9485, 'learning_rate': 0.0001536895152157976, 'epoch': 0.68}         \n",
      "{'loss': 1.0205, 'learning_rate': 0.0001534465826127801, 'epoch': 0.68}         \n",
      "{'loss': 1.0412, 'learning_rate': 0.00015320320765153367, 'epoch': 0.68}        \n",
      "{'loss': 1.2458, 'learning_rate': 0.00015295939234638564, 'epoch': 0.69}        \n",
      "{'loss': 1.0833, 'learning_rate': 0.00015271513871530808, 'epoch': 0.69}        \n",
      "{'loss': 1.3923, 'learning_rate': 0.0001524704487799008, 'epoch': 0.69}         \n",
      "{'loss': 1.0758, 'learning_rate': 0.00015222532456537482, 'epoch': 0.69}        \n",
      "{'loss': 0.9479, 'learning_rate': 0.00015197976810053544, 'epoch': 0.69}        \n",
      "{'loss': 1.1543, 'learning_rate': 0.00015173378141776568, 'epoch': 0.69}        \n",
      "{'loss': 0.8817, 'learning_rate': 0.00015148736655300923, 'epoch': 0.7}         \n",
      "{'loss': 0.8984, 'learning_rate': 0.00015124052554575364, 'epoch': 0.7}         \n",
      "{'loss': 0.9922, 'learning_rate': 0.0001509932604390136, 'epoch': 0.7}          \n",
      "{'loss': 0.8437, 'learning_rate': 0.00015074557327931387, 'epoch': 0.7}         \n",
      "{'loss': 1.2181, 'learning_rate': 0.0001504974661166724, 'epoch': 0.7}          \n",
      "{'loss': 0.9085, 'learning_rate': 0.0001502489410045833, 'epoch': 0.71}         \n",
      "{'loss': 1.1825, 'learning_rate': 0.00015000000000000001, 'epoch': 0.71}        \n",
      "{'loss': 0.9355, 'learning_rate': 0.00014975064516331804, 'epoch': 0.71}        \n",
      "{'loss': 1.2628, 'learning_rate': 0.00014950087855835815, 'epoch': 0.71}        \n",
      "{'loss': 0.9545, 'learning_rate': 0.00014925070225234904, 'epoch': 0.71}        \n",
      "{'loss': 0.951, 'learning_rate': 0.0001490001183159105, 'epoch': 0.71}          \n",
      "{'loss': 1.0906, 'learning_rate': 0.000148749128823036, 'epoch': 0.72}          \n",
      "{'loss': 0.7817, 'learning_rate': 0.00014849773585107575, 'epoch': 0.72}        \n",
      "{'loss': 1.1254, 'learning_rate': 0.00014824594148071934, 'epoch': 0.72}        \n",
      "{'loss': 0.959, 'learning_rate': 0.00014799374779597867, 'epoch': 0.72}         \n",
      "{'loss': 1.2553, 'learning_rate': 0.00014774115688417048, 'epoch': 0.72}        \n",
      "{'loss': 1.1707, 'learning_rate': 0.00014748817083589942, 'epoch': 0.72}        \n",
      "{'loss': 0.943, 'learning_rate': 0.00014723479174504037, 'epoch': 0.73}         \n",
      "{'loss': 1.2472, 'learning_rate': 0.0001469810217087214, 'epoch': 0.73}         \n",
      "{'loss': 1.0261, 'learning_rate': 0.0001467268628273062, 'epoch': 0.73}         \n",
      "{'loss': 0.9446, 'learning_rate': 0.00014647231720437686, 'epoch': 0.73}        \n",
      "{'loss': 0.9742, 'learning_rate': 0.00014621738694671637, 'epoch': 0.73}        \n",
      "{'loss': 1.0895, 'learning_rate': 0.0001459620741642912, 'epoch': 0.74}         \n",
      "{'loss': 1.0944, 'learning_rate': 0.0001457063809702338, 'epoch': 0.74}         \n",
      "{'loss': 0.802, 'learning_rate': 0.00014545030948082517, 'epoch': 0.74}         \n",
      "{'loss': 1.0766, 'learning_rate': 0.0001451938618154775, 'epoch': 0.74}         \n",
      "{'loss': 1.1068, 'learning_rate': 0.00014493704009671613, 'epoch': 0.74}        \n",
      "{'loss': 1.2124, 'learning_rate': 0.00014467984645016258, 'epoch': 0.74}        \n",
      "{'loss': 0.9333, 'learning_rate': 0.00014442228300451656, 'epoch': 0.75}        \n",
      "{'loss': 0.9, 'learning_rate': 0.00014416435189153846, 'epoch': 0.75}           \n",
      "{'loss': 1.0164, 'learning_rate': 0.0001439060552460318, 'epoch': 0.75}         \n",
      "{'loss': 0.9923, 'learning_rate': 0.0001436473952058254, 'epoch': 0.75}         \n",
      "{'loss': 1.3185, 'learning_rate': 0.00014338837391175582, 'epoch': 0.75}        \n",
      "{'loss': 1.07, 'learning_rate': 0.0001431289935076496, 'epoch': 0.75}           \n",
      "{'loss': 0.8041, 'learning_rate': 0.00014286925614030542, 'epoch': 0.76}        \n",
      "{'loss': 1.0678, 'learning_rate': 0.00014260916395947656, 'epoch': 0.76}        \n",
      "{'loss': 0.9824, 'learning_rate': 0.0001423487191178528, 'epoch': 0.76}         \n",
      "{'loss': 1.0, 'learning_rate': 0.0001420879237710429, 'epoch': 0.76}            \n",
      "{'loss': 1.0009, 'learning_rate': 0.0001418267800775565, 'epoch': 0.76}         \n",
      "{'loss': 1.0934, 'learning_rate': 0.0001415652901987866, 'epoch': 0.77}         \n",
      "{'loss': 1.1938, 'learning_rate': 0.00014130345629899114, 'epoch': 0.77}        \n",
      "{'loss': 1.0119, 'learning_rate': 0.0001410412805452757, 'epoch': 0.77}         \n",
      "{'loss': 0.8945, 'learning_rate': 0.00014077876510757502, 'epoch': 0.77}        \n",
      "{'loss': 1.0777, 'learning_rate': 0.00014051591215863545, 'epoch': 0.77}        \n",
      "{'loss': 1.1997, 'learning_rate': 0.00014025272387399674, 'epoch': 0.77}        \n",
      "{'loss': 0.9477, 'learning_rate': 0.00013998920243197407, 'epoch': 0.78}        \n",
      "{'loss': 0.9917, 'learning_rate': 0.00013972535001364014, 'epoch': 0.78}        \n",
      "{'loss': 1.1489, 'learning_rate': 0.00013946116880280681, 'epoch': 0.78}        \n",
      "{'loss': 1.0371, 'learning_rate': 0.00013919666098600753, 'epoch': 0.78}        \n",
      "{'loss': 1.0628, 'learning_rate': 0.0001389318287524787, 'epoch': 0.78}         \n",
      "{'loss': 0.9343, 'learning_rate': 0.0001386666742941419, 'epoch': 0.79}         \n",
      "{'loss': 1.321, 'learning_rate': 0.0001384011998055857, 'epoch': 0.79}          \n",
      "{'loss': 0.8201, 'learning_rate': 0.00013813540748404735, 'epoch': 0.79}        \n",
      "{'loss': 0.9847, 'learning_rate': 0.00013786929952939477, 'epoch': 0.79}        \n",
      "{'loss': 0.898, 'learning_rate': 0.00013760287814410823, 'epoch': 0.79}         \n",
      "{'loss': 1.0789, 'learning_rate': 0.00013733614553326212, 'epoch': 0.79}        \n",
      "{'loss': 1.0037, 'learning_rate': 0.00013706910390450677, 'epoch': 0.8}         \n",
      "{'loss': 1.3337, 'learning_rate': 0.00013680175546805013, 'epoch': 0.8}         \n",
      "{'loss': 1.0439, 'learning_rate': 0.00013653410243663952, 'epoch': 0.8}         \n",
      "{'loss': 1.068, 'learning_rate': 0.0001362661470255432, 'epoch': 0.8}           \n",
      "{'loss': 0.9015, 'learning_rate': 0.00013599789145253227, 'epoch': 0.8}         \n",
      "{'loss': 0.8778, 'learning_rate': 0.00013572933793786193, 'epoch': 0.8}         \n",
      "{'loss': 0.8912, 'learning_rate': 0.00013546048870425356, 'epoch': 0.81}        \n",
      "{'loss': 1.2413, 'learning_rate': 0.00013519134597687603, 'epoch': 0.81}        \n",
      "{'loss': 1.2096, 'learning_rate': 0.00013492191198332724, 'epoch': 0.81}        \n",
      "{'loss': 0.874, 'learning_rate': 0.000134652188953616, 'epoch': 0.81}           \n",
      "{'loss': 0.9735, 'learning_rate': 0.00013438217912014317, 'epoch': 0.81}        \n",
      "{'loss': 1.5938, 'learning_rate': 0.00013411188471768354, 'epoch': 0.82}        \n",
      "{'loss': 1.1837, 'learning_rate': 0.00013384130798336705, 'epoch': 0.82}        \n",
      "{'loss': 1.0994, 'learning_rate': 0.0001335704511566605, 'epoch': 0.82}         \n",
      "{'loss': 0.9595, 'learning_rate': 0.00013329931647934883, 'epoch': 0.82}        \n",
      "{'loss': 0.9608, 'learning_rate': 0.00013302790619551674, 'epoch': 0.82}        \n",
      "{'loss': 1.1896, 'learning_rate': 0.00013275622255152993, 'epoch': 0.82}        \n",
      "{'loss': 0.8833, 'learning_rate': 0.00013248426779601663, 'epoch': 0.83}        \n",
      "{'loss': 0.8892, 'learning_rate': 0.00013221204417984908, 'epoch': 0.83}        \n",
      "{'loss': 1.0448, 'learning_rate': 0.0001319395539561246, 'epoch': 0.83}         \n",
      "{'loss': 0.8555, 'learning_rate': 0.00013166679938014726, 'epoch': 0.83}        \n",
      "{'loss': 0.9834, 'learning_rate': 0.000131393782709409, 'epoch': 0.83}          \n",
      "{'loss': 1.1165, 'learning_rate': 0.0001311205062035711, 'epoch': 0.83}         \n",
      "{'loss': 0.9051, 'learning_rate': 0.00013084697212444531, 'epoch': 0.84}        \n",
      "{'loss': 0.9298, 'learning_rate': 0.0001305731827359753, 'epoch': 0.84}         \n",
      "{'loss': 1.1147, 'learning_rate': 0.0001302991403042178, 'epoch': 0.84}         \n",
      "{'loss': 1.1049, 'learning_rate': 0.0001300248470973239, 'epoch': 0.84}         \n",
      "{'loss': 0.8534, 'learning_rate': 0.00012975030538552032, 'epoch': 0.84}        \n",
      "{'loss': 1.1061, 'learning_rate': 0.00012947551744109043, 'epoch': 0.85}        \n",
      "{'loss': 1.0797, 'learning_rate': 0.00012920048553835572, 'epoch': 0.85}        \n",
      "{'loss': 1.0597, 'learning_rate': 0.00012892521195365678, 'epoch': 0.85}        \n",
      "{'loss': 0.7243, 'learning_rate': 0.00012864969896533447, 'epoch': 0.85}        \n",
      "{'loss': 0.9837, 'learning_rate': 0.00012837394885371125, 'epoch': 0.85}        \n",
      "{'loss': 0.9466, 'learning_rate': 0.00012809796390107195, 'epoch': 0.85}        \n",
      "{'loss': 0.8687, 'learning_rate': 0.0001278217463916453, 'epoch': 0.86}         \n",
      "{'loss': 0.9547, 'learning_rate': 0.0001275452986115847, 'epoch': 0.86}         \n",
      "{'loss': 0.8788, 'learning_rate': 0.00012726862284894938, 'epoch': 0.86}        \n",
      "{'loss': 1.073, 'learning_rate': 0.00012699172139368572, 'epoch': 0.86}         \n",
      "{'loss': 0.7915, 'learning_rate': 0.0001267145965376078, 'epoch': 0.86}         \n",
      "{'loss': 1.1457, 'learning_rate': 0.0001264372505743789, 'epoch': 0.87}         \n",
      "{'loss': 0.9806, 'learning_rate': 0.00012615968579949224, 'epoch': 0.87}        \n",
      "{'loss': 1.1027, 'learning_rate': 0.00012588190451025207, 'epoch': 0.87}        \n",
      "{'loss': 0.6962, 'learning_rate': 0.0001256039090057547, 'epoch': 0.87}         \n",
      "{'loss': 0.8558, 'learning_rate': 0.0001253257015868694, 'epoch': 0.87}         \n",
      "{'loss': 0.9903, 'learning_rate': 0.00012504728455621934, 'epoch': 0.87}        \n",
      "{'loss': 1.0119, 'learning_rate': 0.0001247686602181626, 'epoch': 0.88}         \n",
      "{'loss': 1.0771, 'learning_rate': 0.00012448983087877307, 'epoch': 0.88}        \n",
      "{'loss': 0.8397, 'learning_rate': 0.0001242107988458214, 'epoch': 0.88}         \n",
      "{'loss': 1.3244, 'learning_rate': 0.0001239315664287558, 'epoch': 0.88}         \n",
      "{'loss': 1.0933, 'learning_rate': 0.000123652135938683, 'epoch': 0.88}          \n",
      "{'loss': 0.9802, 'learning_rate': 0.00012337250968834913, 'epoch': 0.88}        \n",
      "{'loss': 1.0795, 'learning_rate': 0.0001230926899921206, 'epoch': 0.89}         \n",
      "{'loss': 1.0179, 'learning_rate': 0.00012281267916596474, 'epoch': 0.89}        \n",
      " 44%|███████████████▉                    | 500/1126 [1:36:09<2:03:47, 11.86s/it]/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.8956, 'learning_rate': 0.000122532479527431, 'epoch': 0.89}          \n",
      "{'loss': 0.9626, 'learning_rate': 0.00012225209339563145, 'epoch': 0.89}        \n",
      "{'loss': 1.112, 'learning_rate': 0.00012197152309122173, 'epoch': 0.89}         \n",
      "{'loss': 0.9528, 'learning_rate': 0.00012169077093638175, 'epoch': 0.9}         \n",
      "{'loss': 0.8965, 'learning_rate': 0.00012140983925479662, 'epoch': 0.9}         \n",
      "{'loss': 1.286, 'learning_rate': 0.00012112873037163728, 'epoch': 0.9}          \n",
      "{'loss': 1.0409, 'learning_rate': 0.0001208474466135413, 'epoch': 0.9}          \n",
      "{'loss': 0.7604, 'learning_rate': 0.00012056599030859366, 'epoch': 0.9}         \n",
      "{'loss': 1.2429, 'learning_rate': 0.00012028436378630741, 'epoch': 0.9}         \n",
      "{'loss': 0.9494, 'learning_rate': 0.00012000256937760445, 'epoch': 0.91}        \n",
      "{'loss': 1.0344, 'learning_rate': 0.00011972060941479621, 'epoch': 0.91}        \n",
      "{'loss': 1.2466, 'learning_rate': 0.00011943848623156435, 'epoch': 0.91}        \n",
      "{'loss': 0.9991, 'learning_rate': 0.00011915620216294145, 'epoch': 0.91}        \n",
      "{'loss': 0.8332, 'learning_rate': 0.00011887375954529168, 'epoch': 0.91}        \n",
      "{'loss': 0.8889, 'learning_rate': 0.00011859116071629149, 'epoch': 0.91}        \n",
      "{'loss': 1.0732, 'learning_rate': 0.00011830840801491014, 'epoch': 0.92}        \n",
      "{'loss': 0.8849, 'learning_rate': 0.0001180255037813906, 'epoch': 0.92}         \n",
      "{'loss': 1.1207, 'learning_rate': 0.00011774245035722983, 'epoch': 0.92}        \n",
      "{'loss': 0.6953, 'learning_rate': 0.00011745925008515976, 'epoch': 0.92}        \n",
      "{'loss': 1.1118, 'learning_rate': 0.00011717590530912763, 'epoch': 0.92}        \n",
      "{'loss': 1.0148, 'learning_rate': 0.00011689241837427665, 'epoch': 0.93}        \n",
      "{'loss': 0.8331, 'learning_rate': 0.00011660879162692675, 'epoch': 0.93}        \n",
      "{'loss': 1.0179, 'learning_rate': 0.00011632502741455496, 'epoch': 0.93}        \n",
      "{'loss': 0.843, 'learning_rate': 0.00011604112808577603, 'epoch': 0.93}         \n",
      "{'loss': 1.1234, 'learning_rate': 0.00011575709599032312, 'epoch': 0.93}        \n",
      "{'loss': 0.7464, 'learning_rate': 0.00011547293347902812, 'epoch': 0.93}        \n",
      "{'loss': 0.8897, 'learning_rate': 0.0001151886429038025, 'epoch': 0.94}         \n",
      "{'loss': 1.0693, 'learning_rate': 0.00011490422661761744, 'epoch': 0.94}        \n",
      "{'loss': 0.9902, 'learning_rate': 0.00011461968697448485, 'epoch': 0.94}        \n",
      "{'loss': 0.9291, 'learning_rate': 0.00011433502632943735, 'epoch': 0.94}        \n",
      "{'loss': 1.0184, 'learning_rate': 0.00011405024703850929, 'epoch': 0.94}        \n",
      "{'loss': 1.0337, 'learning_rate': 0.00011376535145871684, 'epoch': 0.94}        \n",
      "{'loss': 1.0546, 'learning_rate': 0.00011348034194803874, 'epoch': 0.95}        \n",
      "{'loss': 1.0628, 'learning_rate': 0.00011319522086539667, 'epoch': 0.95}        \n",
      "{'loss': 1.0927, 'learning_rate': 0.00011290999057063569, 'epoch': 0.95}        \n",
      "{'loss': 1.2791, 'learning_rate': 0.00011262465342450494, 'epoch': 0.95}        \n",
      "{'loss': 1.0173, 'learning_rate': 0.00011233921178863773, 'epoch': 0.95}        \n",
      "{'loss': 0.8242, 'learning_rate': 0.0001120536680255323, 'epoch': 0.96}         \n",
      "{'loss': 0.8677, 'learning_rate': 0.00011176802449853221, 'epoch': 0.96}        \n",
      "{'loss': 0.8188, 'learning_rate': 0.00011148228357180657, 'epoch': 0.96}        \n",
      "{'loss': 0.8489, 'learning_rate': 0.00011119644761033078, 'epoch': 0.96}        \n",
      "{'loss': 0.8589, 'learning_rate': 0.00011091051897986678, 'epoch': 0.96}        \n",
      "{'loss': 0.9153, 'learning_rate': 0.00011062450004694343, 'epoch': 0.96}        \n",
      "{'loss': 0.8138, 'learning_rate': 0.00011033839317883701, 'epoch': 0.97}        \n",
      "{'loss': 1.0111, 'learning_rate': 0.00011005220074355171, 'epoch': 0.97}        \n",
      "{'loss': 0.8113, 'learning_rate': 0.00010976592510979982, 'epoch': 0.97}        \n",
      "{'loss': 0.8057, 'learning_rate': 0.00010947956864698223, 'epoch': 0.97}        \n",
      "{'loss': 0.9363, 'learning_rate': 0.00010919313372516893, 'epoch': 0.97}        \n",
      "{'loss': 0.8787, 'learning_rate': 0.00010890662271507912, 'epoch': 0.98}        \n",
      "{'loss': 1.172, 'learning_rate': 0.00010862003798806196, 'epoch': 0.98}         \n",
      "{'loss': 0.9985, 'learning_rate': 0.00010833338191607653, 'epoch': 0.98}        \n",
      "{'loss': 1.0416, 'learning_rate': 0.00010804665687167262, 'epoch': 0.98}        \n",
      "{'loss': 1.0218, 'learning_rate': 0.00010775986522797063, 'epoch': 0.98}        \n",
      "{'loss': 0.8245, 'learning_rate': 0.00010747300935864243, 'epoch': 0.98}        \n",
      "{'loss': 0.8855, 'learning_rate': 0.00010632499111669454, 'epoch': 0.99}        \n",
      "{'loss': 0.9793, 'learning_rate': 0.00010603784974222861, 'epoch': 0.99}        \n",
      "{'loss': 0.8829, 'learning_rate': 0.00010575065839464206, 'epoch': 0.99}        \n",
      "{'loss': 1.1365, 'learning_rate': 0.00010546341945091487, 'epoch': 1.0}         \n",
      "{'loss': 0.8739, 'learning_rate': 0.00010517613528842097, 'epoch': 1.0}         \n",
      "{'loss': 0.6118, 'learning_rate': 0.00010488880828490841, 'epoch': 1.0}         \n",
      "{'loss': 0.6795, 'learning_rate': 0.00010460144081848006, 'epoch': 1.0}         \n",
      "{'loss': 0.9328, 'learning_rate': 0.00010431403526757347, 'epoch': 1.0}         \n",
      "{'loss': 0.8758, 'learning_rate': 0.00010402659401094152, 'epoch': 1.01}        \n",
      "{'loss': 0.8634, 'learning_rate': 0.0001037391194276326, 'epoch': 1.01}         \n",
      "{'loss': 0.7079, 'learning_rate': 0.00010345161389697082, 'epoch': 1.01}        \n",
      "{'loss': 0.7917, 'learning_rate': 0.00010316407979853664, 'epoch': 1.01}        \n",
      "{'loss': 0.9433, 'learning_rate': 0.00010287651951214674, 'epoch': 1.01}        \n",
      "{'loss': 1.0566, 'learning_rate': 0.00010258893541783476, 'epoch': 1.01}        \n",
      "{'loss': 0.7686, 'learning_rate': 0.00010230132989583122, 'epoch': 1.02}        \n",
      "{'loss': 0.6622, 'learning_rate': 0.00010201370532654404, 'epoch': 1.02}        \n",
      "{'loss': 0.9506, 'learning_rate': 0.00010172606409053886, 'epoch': 1.02}        \n",
      "{'loss': 0.9449, 'learning_rate': 0.00010143840856851916, 'epoch': 1.02}        \n",
      "{'loss': 0.865, 'learning_rate': 0.00010115074114130677, 'epoch': 1.02}         \n",
      "{'loss': 0.7518, 'learning_rate': 0.0001008630641898219, 'epoch': 1.02}         \n",
      "{'loss': 0.7758, 'learning_rate': 0.00010057538009506377, 'epoch': 1.03}        \n",
      "{'loss': 0.7908, 'learning_rate': 0.00010028769123809057, 'epoch': 1.03}        \n",
      "{'loss': 0.6533, 'learning_rate': 0.0001, 'epoch': 1.03}                        \n",
      "{'loss': 0.6059, 'learning_rate': 9.971230876190946e-05, 'epoch': 1.03}         \n",
      "{'loss': 0.7224, 'learning_rate': 9.942461990493625e-05, 'epoch': 1.03}         \n",
      "{'loss': 0.9722, 'learning_rate': 9.913693581017812e-05, 'epoch': 1.04}         \n",
      "{'loss': 0.886, 'learning_rate': 9.884925885869326e-05, 'epoch': 1.04}          \n",
      "{'loss': 0.6133, 'learning_rate': 9.856159143148086e-05, 'epoch': 1.04}         \n",
      "{'loss': 0.7431, 'learning_rate': 9.827393590946116e-05, 'epoch': 1.04}         \n",
      "{'loss': 0.8634, 'learning_rate': 9.798629467345599e-05, 'epoch': 1.04}         \n",
      "{'loss': 0.7508, 'learning_rate': 9.769867010416882e-05, 'epoch': 1.04}         \n",
      "{'loss': 0.6595, 'learning_rate': 9.741106458216528e-05, 'epoch': 1.05}         \n",
      "{'loss': 0.7386, 'learning_rate': 9.712348048785329e-05, 'epoch': 1.05}         \n",
      "{'loss': 0.6036, 'learning_rate': 9.683592020146341e-05, 'epoch': 1.05}         \n",
      "{'loss': 1.119, 'learning_rate': 9.654838610302923e-05, 'epoch': 1.05}          \n",
      "{'loss': 0.9337, 'learning_rate': 9.626088057236745e-05, 'epoch': 1.05}         \n",
      "{'loss': 0.8066, 'learning_rate': 9.597340598905852e-05, 'epoch': 1.06}         \n",
      "{'loss': 0.7523, 'learning_rate': 9.568596473242654e-05, 'epoch': 1.06}         \n",
      "{'loss': 0.926, 'learning_rate': 9.539855918151997e-05, 'epoch': 1.06}          \n",
      "{'loss': 0.9875, 'learning_rate': 9.511119171509161e-05, 'epoch': 1.06}         \n",
      "{'loss': 0.7134, 'learning_rate': 9.482386471157904e-05, 'epoch': 1.06}         \n",
      "{'loss': 0.7628, 'learning_rate': 9.453658054908511e-05, 'epoch': 1.06}         \n",
      " 53%|███████████████████▏                | 599/1126 [1:55:01<1:43:29, 11.78s/it]"
     ]
    }
   ],
   "source": [
    "!python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
    "    --lora_enable True \\\n",
    "    --lora_r 16 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --mm_projector_lr 2e-5 \\\n",
    "    --model_name_or_path Lin-Chen/ShareGPT4V-7B \\\n",
    "    --version v1 \\\n",
    "    --data_path /home/jupyter/ShareGPT4V/datasets/train_samples_tuning_1p.json \\\n",
    "    --image_folder /home/jupyter/art_images_data \\\n",
    "    --vision_tower Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --output_dir ./checkpoints/llava-share4v-lora \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --bits 4 \\\n",
    "    --quant_type fp4 \\\n",
    "    --fp16 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657828a-cdaf-4caf-810d-e6109e464751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Poetry (Local)",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
