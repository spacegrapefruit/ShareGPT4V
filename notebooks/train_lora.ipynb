{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5658e08-3c3b-4927-a522-a2cf83c06e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb68750b-b6e9-4787-8bf4-6869673d299c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = \"\"\"\n",
    "python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
    "    --model_name_or_path \"Lin-Chen/ShareGPT4V-7B\" \\\n",
    "    --version \"vicuna_v1\" \\  # Or other appropriate conversation template version\n",
    "    --data_path \"/home/jupyter/ShareGPT4V/train_samples_tuning.json\" \\\n",
    "    --output_dir \"path/to/your/output_lora_adapters\" \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 2e-4 \\ # This LR will be for any non-LoRA params if tuned. LoRA params scale with alpha/r.\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    # TODO --tf32 False \\ # Set to True if using Ampere GPUs or newer for speed\n",
    "    # TODO --bf16 True \\  # Or --fp16 True, depending on your hardware. BF16 is generally preferred.\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    \\\n",
    "    # --- LoRA Specific Arguments ---\n",
    "    --lora_enable True \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 16 \\ # Or lora_r value\n",
    "    --lora_dropout 0.05 \\\n",
    "    # --lora_bias \"none\" # Default is \"none\"\n",
    "    \\\n",
    "    # --- Multimodal Specific Arguments (if applicable) ---\n",
    "    --vision_tower \"Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\" \\\n",
    "    --image_folder \"/home/jupyter/art_images_data/images\" \\\n",
    "    --is_multimodal True \\\n",
    "    --image_aspect_ratio 'pad' \\\n",
    "    --mm_projector_type 'mlp2x_gelu' \\ # Or other projector type\n",
    "    # TODO --tune_mm_mlp_adapter True \\ # Option to tune the projector alongside LoRA\n",
    "    # --tune_vision_tower True # Option to tune the vision tower (can be memory intensive)\n",
    "    # --vision_tower_lr 2e-5 # If tuning vision tower, specify its LR\n",
    "    # --mm_projector_lr 2e-5 # If tuning MLP adapter, specify its LR\n",
    "    \\\n",
    "    # --- QLoRA Specific Arguments (if using quantization) ---\n",
    "    --bits 4 \\\n",
    "    # TODO --double_quant True \\\n",
    "    --quant_type \"nf4\" \\\n",
    "    # TODO extra params\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7696d2bc-d36f-4c64-8711-c8d1cc7badc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = cmd.split(\"    \")\n",
    "lines = [\n",
    "    re.sub(r\"\\s*#.*\", \"\", l).strip().strip(\"\\\\\").strip()\n",
    "    for l in lines\n",
    "]\n",
    "lines = [l for l in lines if l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b95d1a7-1a49-4d26-9896-5ac64147652d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
      "    --model_name_or_path \"Lin-Chen/ShareGPT4V-7B\" \\\n",
      "    --version \"vicuna_v1\" \\\n",
      "    --data_path \"/home/jupyter/ShareGPT4V/train_samples_tuning.json\" \\\n",
      "    --output_dir \"path/to/your/output_lora_adapters\" \\\n",
      "    --num_train_epochs 3 \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 8 \\\n",
      "    --learning_rate 2e-4 \\\n",
      "    --weight_decay 0. \\\n",
      "    --warmup_ratio 0.03 \\\n",
      "    --lr_scheduler_type \"cosine\" \\\n",
      "    --logging_steps 1 \\\n",
      "    --model_max_length 2048 \\\n",
      "    --gradient_checkpointing True \\\n",
      "    --lazy_preprocess True \\\n",
      "    --lora_enable True \\\n",
      "    --lora_r 64 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --vision_tower \"Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\" \\\n",
      "    --image_folder \"/home/jupyter/art_images_data/images\" \\\n",
      "    --is_multimodal True \\\n",
      "    --image_aspect_ratio 'pad' \\\n",
      "    --mm_projector_type 'mlp2x_gelu' \\\n",
      "    --bits 4 \\\n",
      "    --quant_type \"nf4\"\n"
     ]
    }
   ],
   "source": [
    "print(\"!\" + \" \\\\\\n    \".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a074194-f8af-44a2-bf43-21a004e47aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.41s/it]\n",
      "Adding LoRA adapters...\n",
      "Load vision tower from Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12\n",
      "Total parameters: 3,854,391,296\n",
      "Trainable parameters: 39,976,960\n",
      "Frozen parameters: 3,814,414,336\n",
      "Trainable%: 1.04%\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:427: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/accelerate/accelerator.py:1301: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  new_forward = torch.cuda.amp.autocast(dtype=torch.float16)(model_forward_func)\n",
      "  0%|                                                  | 0/1126 [00:00<?, ?it/s]/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/.cache/pypoetry/virtualenvs/share4v-KfdsAbnj-py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.7837, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.8455, 'learning_rate': 1.1764705882352942e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.692, 'learning_rate': 1.7647058823529414e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6778, 'learning_rate': 2.3529411764705884e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.8235, 'learning_rate': 2.9411764705882354e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7292, 'learning_rate': 3.529411764705883e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.7433, 'learning_rate': 4.11764705882353e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.486, 'learning_rate': 4.705882352941177e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.5769, 'learning_rate': 5.294117647058824e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5639, 'learning_rate': 5.882352941176471e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.697, 'learning_rate': 6.470588235294118e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.5548, 'learning_rate': 7.058823529411765e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.3633, 'learning_rate': 7.647058823529411e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5767, 'learning_rate': 8.23529411764706e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.5143, 'learning_rate': 8.823529411764706e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.2147, 'learning_rate': 9.411764705882353e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.204, 'learning_rate': 0.0001, 'epoch': 0.03}                         \n",
      "{'loss': 3.1603, 'learning_rate': 0.00010588235294117647, 'epoch': 0.03}        \n",
      "{'loss': 2.9356, 'learning_rate': 0.00011176470588235294, 'epoch': 0.03}        \n",
      "{'loss': 3.0381, 'learning_rate': 0.00011764705882352942, 'epoch': 0.04}        \n",
      "{'loss': 2.9477, 'learning_rate': 0.0001235294117647059, 'epoch': 0.04}         \n",
      "{'loss': 2.7572, 'learning_rate': 0.00012941176470588237, 'epoch': 0.04}        \n",
      "{'loss': 2.7846, 'learning_rate': 0.00013529411764705884, 'epoch': 0.04}        \n",
      "{'loss': 2.5682, 'learning_rate': 0.0001411764705882353, 'epoch': 0.04}         \n",
      "{'loss': 2.8327, 'learning_rate': 0.00014705882352941178, 'epoch': 0.04}        \n",
      "{'loss': 2.6237, 'learning_rate': 0.00015294117647058822, 'epoch': 0.05}        \n",
      "{'loss': 2.9248, 'learning_rate': 0.0001588235294117647, 'epoch': 0.05}         \n",
      "{'loss': 2.924, 'learning_rate': 0.0001647058823529412, 'epoch': 0.05}          \n",
      "{'loss': 2.4469, 'learning_rate': 0.00017058823529411766, 'epoch': 0.05}        \n",
      "{'loss': 2.6377, 'learning_rate': 0.00017647058823529413, 'epoch': 0.05}        \n",
      "{'loss': 2.4871, 'learning_rate': 0.0001823529411764706, 'epoch': 0.06}         \n",
      "{'loss': 2.3812, 'learning_rate': 0.00018823529411764707, 'epoch': 0.06}        \n",
      "{'loss': 2.3321, 'learning_rate': 0.00019411764705882354, 'epoch': 0.06}        \n",
      "{'loss': 2.2651, 'learning_rate': 0.0002, 'epoch': 0.06}                        \n",
      "{'loss': 2.5159, 'learning_rate': 0.00019999958616790135, 'epoch': 0.06}        \n",
      "{'loss': 2.2725, 'learning_rate': 0.00019999834467503052, 'epoch': 0.06}        \n",
      "{'loss': 2.0968, 'learning_rate': 0.00019999627553166294, 'epoch': 0.07}        \n",
      "{'loss': 2.3062, 'learning_rate': 0.0001999933787549241, 'epoch': 0.07}         \n",
      "{'loss': 1.8923, 'learning_rate': 0.00019998965436878962, 'epoch': 0.07}        \n",
      "{'loss': 2.2325, 'learning_rate': 0.00019998510240408496, 'epoch': 0.07}        \n",
      "{'loss': 2.2157, 'learning_rate': 0.00019997972289848503, 'epoch': 0.07}        \n",
      "{'loss': 2.0689, 'learning_rate': 0.00019997351589651408, 'epoch': 0.07}        \n",
      "{'loss': 1.877, 'learning_rate': 0.0001999664814495453, 'epoch': 0.08}          \n",
      "{'loss': 1.9865, 'learning_rate': 0.00019995861961580024, 'epoch': 0.08}        \n",
      "{'loss': 2.0974, 'learning_rate': 0.00019994993046034846, 'epoch': 0.08}        \n",
      "{'loss': 1.9703, 'learning_rate': 0.00019994041405510705, 'epoch': 0.08}        \n",
      "{'loss': 2.1333, 'learning_rate': 0.00019993007047883988, 'epoch': 0.08}        \n",
      "{'loss': 1.9201, 'learning_rate': 0.00019991889981715698, 'epoch': 0.09}        \n",
      "{'loss': 2.269, 'learning_rate': 0.00019990690216251396, 'epoch': 0.09}         \n",
      "{'loss': 1.8182, 'learning_rate': 0.00019989407761421109, 'epoch': 0.09}        \n",
      "{'loss': 1.8492, 'learning_rate': 0.00019988042627839257, 'epoch': 0.09}        \n",
      "{'loss': 1.6813, 'learning_rate': 0.0001998659482680456, 'epoch': 0.09}         \n",
      "{'loss': 1.8707, 'learning_rate': 0.00019985064370299957, 'epoch': 0.09}        \n",
      "{'loss': 1.8502, 'learning_rate': 0.00019983451270992478, 'epoch': 0.1}         \n",
      "{'loss': 1.6953, 'learning_rate': 0.00019981755542233177, 'epoch': 0.1}         \n",
      "{'loss': 2.141, 'learning_rate': 0.00019979977198056984, 'epoch': 0.1}          \n",
      "{'loss': 1.9418, 'learning_rate': 0.00019978116253182627, 'epoch': 0.1}         \n",
      "{'loss': 2.1212, 'learning_rate': 0.0001997617272301248, 'epoch': 0.1}          \n",
      "{'loss': 1.6221, 'learning_rate': 0.00019974146623632434, 'epoch': 0.1}         \n",
      "{'loss': 1.681, 'learning_rate': 0.00019972037971811802, 'epoch': 0.11}         \n",
      "{'loss': 1.7769, 'learning_rate': 0.00019969846785003134, 'epoch': 0.11}        \n",
      "{'loss': 1.8725, 'learning_rate': 0.00019967573081342103, 'epoch': 0.11}        \n",
      "{'loss': 1.591, 'learning_rate': 0.00019965216879647333, 'epoch': 0.11}         \n",
      "{'loss': 1.5443, 'learning_rate': 0.00019962778199420265, 'epoch': 0.11}        \n",
      "{'loss': 1.5311, 'learning_rate': 0.00019960257060844984, 'epoch': 0.12}        \n",
      "{'loss': 2.0186, 'learning_rate': 0.00019957653484788053, 'epoch': 0.12}        \n",
      "{'loss': 1.7157, 'learning_rate': 0.00019954967492798333, 'epoch': 0.12}        \n",
      "{'loss': 1.9252, 'learning_rate': 0.0001995219910710682, 'epoch': 0.12}         \n",
      "{'loss': 1.7138, 'learning_rate': 0.00019949348350626456, 'epoch': 0.12}        \n",
      "{'loss': 1.6093, 'learning_rate': 0.0001994641524695193, 'epoch': 0.12}         \n",
      "{'loss': 1.6183, 'learning_rate': 0.00019943399820359484, 'epoch': 0.13}        \n",
      "{'loss': 1.615, 'learning_rate': 0.0001994030209580673, 'epoch': 0.13}          \n",
      "{'loss': 1.618, 'learning_rate': 0.00019937122098932428, 'epoch': 0.13}         \n",
      "{'loss': 1.5668, 'learning_rate': 0.00019933859856056265, 'epoch': 0.13}        \n",
      "{'loss': 1.6148, 'learning_rate': 0.0001993051539417867, 'epoch': 0.13}         \n",
      "{'loss': 1.8238, 'learning_rate': 0.0001992708874098054, 'epoch': 0.13}         \n",
      "{'loss': 1.6852, 'learning_rate': 0.0001992357992482307, 'epoch': 0.14}         \n",
      "{'loss': 1.9684, 'learning_rate': 0.00019919988974747473, 'epoch': 0.14}        \n",
      "{'loss': 1.5783, 'learning_rate': 0.0001991631592047475, 'epoch': 0.14}         \n",
      "{'loss': 1.8055, 'learning_rate': 0.00019912560792405463, 'epoch': 0.14}        \n",
      "{'loss': 1.342, 'learning_rate': 0.00019908723621619464, 'epoch': 0.14}         \n",
      "{'loss': 1.7627, 'learning_rate': 0.00019904804439875633, 'epoch': 0.15}        \n",
      "{'loss': 1.7447, 'learning_rate': 0.0001990080327961164, 'epoch': 0.15}         \n",
      "{'loss': 1.72, 'learning_rate': 0.00019896720173943658, 'epoch': 0.15}          \n",
      "{'loss': 1.7329, 'learning_rate': 0.00019892555156666089, 'epoch': 0.15}        \n",
      "{'loss': 1.775, 'learning_rate': 0.00019888308262251285, 'epoch': 0.15}         \n",
      "{'loss': 1.572, 'learning_rate': 0.0001988397952584928, 'epoch': 0.15}          \n",
      "{'loss': 1.5087, 'learning_rate': 0.00019879568983287467, 'epoch': 0.16}        \n",
      "{'loss': 1.5362, 'learning_rate': 0.0001987507667107033, 'epoch': 0.16}         \n",
      "{'loss': 1.7458, 'learning_rate': 0.00019870502626379127, 'epoch': 0.16}        \n",
      "{'loss': 1.6077, 'learning_rate': 0.00019865846887071596, 'epoch': 0.16}        \n",
      "{'loss': 1.5135, 'learning_rate': 0.00019861109491681617, 'epoch': 0.16}        \n",
      "{'loss': 1.6719, 'learning_rate': 0.00019856290479418917, 'epoch': 0.17}        \n",
      "{'loss': 1.7293, 'learning_rate': 0.0001985138989016874, 'epoch': 0.17}         \n",
      "{'loss': 1.8113, 'learning_rate': 0.00019846407764491502, 'epoch': 0.17}        \n",
      "{'loss': 1.7285, 'learning_rate': 0.00019841344143622474, 'epoch': 0.17}        \n",
      "{'loss': 1.6788, 'learning_rate': 0.00019836199069471437, 'epoch': 0.17}        \n",
      "{'loss': 1.548, 'learning_rate': 0.00019830972584622324, 'epoch': 0.17}         \n",
      "{'loss': 1.4557, 'learning_rate': 0.00019825664732332884, 'epoch': 0.18}        \n",
      "{'loss': 1.3257, 'learning_rate': 0.00019820275556534304, 'epoch': 0.18}        \n",
      "{'loss': 1.6563, 'learning_rate': 0.00019814805101830865, 'epoch': 0.18}        \n",
      "{'loss': 1.5756, 'learning_rate': 0.00019809253413499565, 'epoch': 0.18}        \n",
      "{'loss': 1.4633, 'learning_rate': 0.00019803620537489736, 'epoch': 0.18}        \n",
      "{'loss': 1.2755, 'learning_rate': 0.00019797906520422677, 'epoch': 0.18}        \n",
      "{'loss': 1.2956, 'learning_rate': 0.00019792111409591264, 'epoch': 0.19}        \n",
      "{'loss': 1.4402, 'learning_rate': 0.00019786235252959553, 'epoch': 0.19}        \n",
      "{'loss': 1.3877, 'learning_rate': 0.0001978027809916239, 'epoch': 0.19}         \n",
      "{'loss': 1.4068, 'learning_rate': 0.00019774239997505, 'epoch': 0.19}           \n",
      "{'loss': 1.3806, 'learning_rate': 0.00019768120997962592, 'epoch': 0.19}        \n",
      "{'loss': 1.6541, 'learning_rate': 0.00019761921151179937, 'epoch': 0.2}         \n",
      "{'loss': 1.5127, 'learning_rate': 0.00019755640508470942, 'epoch': 0.2}         \n",
      "{'loss': 1.4471, 'learning_rate': 0.00019749279121818235, 'epoch': 0.2}         \n",
      "{'loss': 1.644, 'learning_rate': 0.00019742837043872747, 'epoch': 0.2}          \n",
      "{'loss': 1.3223, 'learning_rate': 0.00019736314327953243, 'epoch': 0.2}         \n",
      "{'loss': 1.2397, 'learning_rate': 0.00019729711028045909, 'epoch': 0.2}         \n",
      "{'loss': 1.3737, 'learning_rate': 0.00019723027198803895, 'epoch': 0.21}        \n",
      "{'loss': 1.5426, 'learning_rate': 0.00019716262895546857, 'epoch': 0.21}        \n",
      "{'loss': 1.6417, 'learning_rate': 0.0001970941817426052, 'epoch': 0.21}         \n",
      "{'loss': 1.5148, 'learning_rate': 0.00019702493091596187, 'epoch': 0.21}        \n",
      "{'loss': 1.7839, 'learning_rate': 0.0001969548770487029, 'epoch': 0.21}         \n",
      "{'loss': 1.3935, 'learning_rate': 0.00019688402072063903, 'epoch': 0.21}        \n",
      "{'loss': 1.2891, 'learning_rate': 0.00019681236251822273, 'epoch': 0.22}        \n",
      "{'loss': 1.4904, 'learning_rate': 0.00019673990303454333, 'epoch': 0.22}        \n",
      "{'loss': 1.2641, 'learning_rate': 0.00019666664286932198, 'epoch': 0.22}        \n",
      "{'loss': 1.6664, 'learning_rate': 0.00019659258262890683, 'epoch': 0.22}        \n",
      "{'loss': 1.5412, 'learning_rate': 0.00019651772292626803, 'epoch': 0.22}        \n",
      "{'loss': 1.4619, 'learning_rate': 0.0001964420643809925, 'epoch': 0.23}         \n",
      "{'loss': 1.3616, 'learning_rate': 0.00019636560761927897, 'epoch': 0.23}        \n",
      "{'loss': 1.2575, 'learning_rate': 0.00019628835327393262, 'epoch': 0.23}        \n",
      "{'loss': 1.6226, 'learning_rate': 0.00019621030198436006, 'epoch': 0.23}        \n",
      "{'loss': 1.2949, 'learning_rate': 0.00019613145439656386, 'epoch': 0.23}        \n",
      "{'loss': 1.2743, 'learning_rate': 0.00019605181116313724, 'epoch': 0.23}        \n",
      "{'loss': 1.3383, 'learning_rate': 0.00019597137294325877, 'epoch': 0.24}        \n",
      "{'loss': 1.4572, 'learning_rate': 0.00019589014040268676, 'epoch': 0.24}        \n",
      "{'loss': 1.1204, 'learning_rate': 0.0001958081142137539, 'epoch': 0.24}         \n",
      "{'loss': 1.4496, 'learning_rate': 0.0001957252950553616, 'epoch': 0.24}         \n",
      "{'loss': 1.2679, 'learning_rate': 0.0001956416836129743, 'epoch': 0.24}         \n",
      "{'loss': 1.4798, 'learning_rate': 0.0001955572805786141, 'epoch': 0.25}         \n",
      "{'loss': 1.2979, 'learning_rate': 0.00019547208665085457, 'epoch': 0.25}        \n",
      "{'loss': 1.6611, 'learning_rate': 0.0001953861025348154, 'epoch': 0.25}         \n",
      "{'loss': 1.2781, 'learning_rate': 0.00019529932894215638, 'epoch': 0.25}        \n",
      "{'loss': 1.3171, 'learning_rate': 0.00019521176659107142, 'epoch': 0.25}        \n",
      "{'loss': 1.1517, 'learning_rate': 0.00019512341620628277, 'epoch': 0.25}        \n",
      "{'loss': 1.2881, 'learning_rate': 0.0001950342785190349, 'epoch': 0.26}         \n",
      "{'loss': 1.2893, 'learning_rate': 0.00019494435426708855, 'epoch': 0.26}        \n",
      "{'loss': 1.6329, 'learning_rate': 0.00019485364419471454, 'epoch': 0.26}        \n",
      "{'loss': 1.2629, 'learning_rate': 0.00019476214905268772, 'epoch': 0.26}        \n",
      "{'loss': 1.5961, 'learning_rate': 0.0001946698695982806, 'epoch': 0.26}         \n",
      "{'loss': 1.2241, 'learning_rate': 0.00019457680659525715, 'epoch': 0.26}        \n",
      "{'loss': 1.6972, 'learning_rate': 0.00019448296081386656, 'epoch': 0.27}        \n",
      "{'loss': 1.4103, 'learning_rate': 0.00019438833303083678, 'epoch': 0.27}        \n",
      "{'loss': 1.5213, 'learning_rate': 0.00019429292402936804, 'epoch': 0.27}        \n",
      "{'loss': 1.3887, 'learning_rate': 0.0001941967345991265, 'epoch': 0.27}         \n",
      "{'loss': 1.5547, 'learning_rate': 0.00019409976553623766, 'epoch': 0.27}        \n",
      "{'loss': 1.144, 'learning_rate': 0.0001940020176432797, 'epoch': 0.28}          \n",
      "{'loss': 1.1539, 'learning_rate': 0.00019390349172927697, 'epoch': 0.28}        \n",
      "{'loss': 1.3391, 'learning_rate': 0.00019380418860969322, 'epoch': 0.28}        \n",
      "{'loss': 1.4514, 'learning_rate': 0.00019370410910642471, 'epoch': 0.28}        \n",
      "{'loss': 1.2951, 'learning_rate': 0.00019360325404779374, 'epoch': 0.28}        \n",
      "{'loss': 1.4694, 'learning_rate': 0.0001935016242685415, 'epoch': 0.28}         \n",
      "{'loss': 1.4893, 'learning_rate': 0.0001933992206098213, 'epoch': 0.29}         \n",
      "{'loss': 1.2181, 'learning_rate': 0.0001932960439191915, 'epoch': 0.29}         \n",
      "{'loss': 1.0587, 'learning_rate': 0.0001931920950506087, 'epoch': 0.29}         \n",
      "{'loss': 1.4392, 'learning_rate': 0.00019308737486442045, 'epoch': 0.29}        \n",
      "{'loss': 1.5485, 'learning_rate': 0.00019298188422735822, 'epoch': 0.29}        \n",
      "{'loss': 1.4409, 'learning_rate': 0.00019287562401253022, 'epoch': 0.29}        \n",
      "{'loss': 1.4549, 'learning_rate': 0.0001927685950994143, 'epoch': 0.3}          \n",
      "{'loss': 1.2619, 'learning_rate': 0.0001926607983738504, 'epoch': 0.3}          \n",
      "{'loss': 1.9062, 'learning_rate': 0.00019255223472803334, 'epoch': 0.3}         \n",
      "{'loss': 1.5323, 'learning_rate': 0.00019244290506050568, 'epoch': 0.3}         \n",
      "{'loss': 1.2479, 'learning_rate': 0.00019233281027614986, 'epoch': 0.3}         \n",
      "{'loss': 1.4474, 'learning_rate': 0.00019222195128618106, 'epoch': 0.31}        \n",
      "{'loss': 1.5561, 'learning_rate': 0.00019211032900813934, 'epoch': 0.31}        \n",
      "{'loss': 1.3303, 'learning_rate': 0.00019199794436588243, 'epoch': 0.31}        \n",
      "{'loss': 1.6317, 'learning_rate': 0.00019188479828957772, 'epoch': 0.31}        \n",
      "{'loss': 1.1792, 'learning_rate': 0.00019177089171569482, 'epoch': 0.31}        \n",
      "{'loss': 1.7909, 'learning_rate': 0.00019165622558699763, 'epoch': 0.31}        \n",
      "{'loss': 1.3995, 'learning_rate': 0.00019154080085253666, 'epoch': 0.32}        \n",
      "{'loss': 1.1778, 'learning_rate': 0.00019142461846764105, 'epoch': 0.32}        \n",
      "{'loss': 0.9725, 'learning_rate': 0.0001913076793939109, 'epoch': 0.32}         \n",
      "{'loss': 1.4461, 'learning_rate': 0.00019118998459920902, 'epoch': 0.32}        \n",
      "{'loss': 1.2593, 'learning_rate': 0.00019107153505765306, 'epoch': 0.32}        \n",
      "{'loss': 1.234, 'learning_rate': 0.0001909523317496075, 'epoch': 0.33}          \n",
      "{'loss': 1.2666, 'learning_rate': 0.0001908323756616754, 'epoch': 0.33}         \n",
      "{'loss': 1.6355, 'learning_rate': 0.0001907116677866904, 'epoch': 0.33}         \n",
      "{'loss': 1.579, 'learning_rate': 0.00019059020912370834, 'epoch': 0.33}         \n",
      "{'loss': 1.4525, 'learning_rate': 0.0001904680006779991, 'epoch': 0.33}         \n",
      "{'loss': 1.3757, 'learning_rate': 0.00019034504346103823, 'epoch': 0.33}        \n",
      "{'loss': 1.0238, 'learning_rate': 0.0001902213384904986, 'epoch': 0.34}         \n",
      "{'loss': 1.1903, 'learning_rate': 0.0001900968867902419, 'epoch': 0.34}         \n",
      "{'loss': 1.2577, 'learning_rate': 0.0001899716893903104, 'epoch': 0.34}         \n",
      "{'loss': 1.5611, 'learning_rate': 0.00018984574732691808, 'epoch': 0.34}        \n",
      "{'loss': 1.1926, 'learning_rate': 0.00018971906164244232, 'epoch': 0.34}        \n",
      "{'loss': 1.2081, 'learning_rate': 0.00018959163338541518, 'epoch': 0.34}        \n",
      " 17%|██████▌                               | 194/1126 [37:23<2:52:42, 11.12s/it]"
     ]
    }
   ],
   "source": [
    "!python /home/jupyter/ShareGPT4V/share4v/train/train.py \\\n",
    "    --lora_enable True \\\n",
    "    --lora_r 16 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --mm_projector_lr 2e-5 \\\n",
    "    --model_name_or_path Lin-Chen/ShareGPT4V-7B \\\n",
    "    --version v1 \\\n",
    "    --data_path /home/jupyter/ShareGPT4V/datasets/train_samples_tuning_1p.json \\\n",
    "    --image_folder /home/jupyter/art_images_data \\\n",
    "    --vision_tower Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --output_dir ./checkpoints/llava-share4v-lora \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --bits 4 \\\n",
    "    --quant_type fp4 \\\n",
    "    --fp16 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657828a-cdaf-4caf-810d-e6109e464751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Poetry (Local)",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
