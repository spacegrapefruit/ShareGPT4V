{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95140098",
   "metadata": {},
   "source": [
    "# ShareGPT4V-7B Model Analysis and Exploration\n",
    "\n",
    "This notebook explores the architecture and capabilities of the ShareGPT4V-7B model, a multimodal model that can process both text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a30633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from share4v.model.builder import load_pretrained_model\n",
    "from share4v.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "\n",
    "from share4v.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88e739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Model Loading and Configuration\n",
    "\n",
    "Let's load the pretrained ShareGPT4V-7B model with its components:\n",
    "- Tokenizer: Converts text to tokens\n",
    "- Model: The main neural network architecture\n",
    "- Image processor: Handles image preprocessing\n",
    "- Context length: Maximum sequence length the model can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249f655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model configuration parameters\n",
    "model_path = \"Lin-Chen/ShareGPT4V-7B\"  # HuggingFace repository path\n",
    "model_name = \"share4v-7b\"               # Model identifier\n",
    "\n",
    "# Load the pretrained model and its components\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name, False, False\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully. Context length: {context_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45256a3d",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Overview\n",
    "\n",
    "Let's inspect the high-level architecture of the model to understand its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c8cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5e175",
   "metadata": {},
   "source": [
    "## 3. Text Processing Pipeline\n",
    "\n",
    "Now, we'll explore how the model processes text input. This involves:\n",
    "1. Converting text to token IDs\n",
    "2. Examining token representations\n",
    "3. Visualizing embeddings\n",
    "\n",
    "This helps us understand what the model \"sees\" when processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89c585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple text prompt\n",
    "prompt = \"Tell me a joke about programming.\"\n",
    "# Alternative prompt with image: \"Tell me something interesting about this image: <image>\"\n",
    "stop_str = \"<image>\"  # Special token for stopping generation\n",
    "\n",
    "# Tokenize the input prompt\n",
    "# tokenizer_image_token handles special image tokens if present\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "\n",
    "# Print the tokenized shape\n",
    "print(f\"Tokenized shape: {input_ids.shape}\")\n",
    "\n",
    "# Add a batch dimension and move tensor to the model's device\n",
    "input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "print(f\"Input shape after batching: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d676982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary size for reference\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Display the actual tensor of token IDs\n",
    "print(\"Token IDs tensor:\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdda4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decode each token ID to see how the text was tokenized\n",
    "print(f\"Tokenized representation of: '{prompt}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for token_id in input_ids[0]:\n",
    "    if token_id < 0:\n",
    "        print(f\"Token {token_id:5}: RESERVED_TOKEN\")  # Special tokens have negative IDs\n",
    "        continue\n",
    "    print(f\"Token {token_id:5}: '{tokenizer.decode(token_id)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb749a09",
   "metadata": {},
   "source": [
    "## 4. Image Processing Pipeline\n",
    "\n",
    "Next, we'll explore how the model processes image input:\n",
    "1. Loading and preprocessing an image\n",
    "2. Converting it to tensor format\n",
    "3. Moving it to the appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42daf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "image_path = \"../examples/photo.png\"  # Change this path to your desired image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9856b-5c4d-4268-afc3-0e992fa0e93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "image = Image.open(\"../examples/photo.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05faa3f-5027-480b-8be9-647c6ed3c9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202898d-6e5d-4cac-9af0-c2e3c310af2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f3b3a-c679-4250-b4ce-55787d8dabea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = image.resize((100,762))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3236c-f3f4-4fa0-9623-8d91cb49ca28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pimage = process_images([image, img, image.resize((200,50))], image_processor, model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd4448-34cc-419a-a74d-9985fd6da173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6580c2-d09b-40b9-92a7-4118be77b390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pimage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc88fe-6e65-4cbc-9530-93f30fcefb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    \"\"\"\n",
    "    Convert a CLIP-processed tensor back to a PIL image.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The processed image tensor [C, H, W] (normalized, rescaled)\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The reconstructed PIL image\n",
    "    \"\"\"\n",
    "    # Make sure tensor is on CPU and create a copy to avoid modifying the original\n",
    "    tensor = tensor.cpu().detach().clone()\n",
    "    \n",
    "    \n",
    "    # Image mean and std from the CLIP processor\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(-1, 1, 1)\n",
    "    \n",
    "    # Undo normalization\n",
    "    tensor = tensor * std + mean\n",
    "    \n",
    "    # Undo rescaling (multiply by 1/rescale_factor)\n",
    "    tensor = tensor * (1 / 0.00392156862745098)\n",
    "    \n",
    "    # Clamp values to valid image range [0, 255]\n",
    "    tensor = torch.clamp(tensor, 0, 255)\n",
    "    \n",
    "    # Convert to numpy array and correct data type\n",
    "    image_np = tensor.numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "    \n",
    "    # Create PIL image\n",
    "    pil_image = Image.fromarray(image_np)\n",
    "    \n",
    "    return pil_image\n",
    "\n",
    "tensor_to_pil_image(pimage[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827375b-b781-42de-b77c-c536c4f46f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tensor_to_pil_image(pimage[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3edea-f436-4d8a-8831-86aa7448817a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tensor_to_pil_image(pimage[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "images = [\n",
    "    Image.open(\"../examples/photo.png\"),\n",
    "]\n",
    "print(f\"Loaded image with size: {images[0].size}\")\n",
    "\n",
    "# Process images using ShareGPT4V's image processor\n",
    "# This includes resizing, normalization, and other transformations\n",
    "images = process_images(images, image_processor, model.config)\n",
    "\n",
    "# Ensure images are in the right format and on the correct device\n",
    "if isinstance(images, list):\n",
    "    images = [img.to(model.device, dtype=torch.float32) for img in images]\n",
    "else:\n",
    "    images = images.to(model.device, dtype=torch.float32)\n",
    "\n",
    "print(f\"Processed image tensor shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c558d-7afb-46c5-b1a1-26c00a341a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef83554",
   "metadata": {},
   "source": [
    "## 5. Vision Model Analysis\n",
    "\n",
    "Now we'll examine how the model processes images through its vision tower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8587468",
   "metadata": {},
   "source": [
    "### 5.1 Vision Tower Architecture\n",
    "\n",
    "First, let's look at the structure of the vision components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af252125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine the vision tower's embedding components\n",
    "print(\"Vision embedding components:\")\n",
    "print(model.model.vision_tower.vision_tower.vision_model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893f95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore the patch embedding process\n",
    "# The image is divided into patches that are individually embedded\n",
    "patch_embeddings = model.model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding(images)\n",
    "print(f\"Patch embedding shape: {patch_embeddings.shape}\")\n",
    "# This shows how the image is divided into spatial patches and projected to the embedding space\n",
    "\n",
    "# Get the full vision embeddings (patches + position embeddings)\n",
    "vision_embeddings = model.model.vision_tower.vision_tower.vision_model.embeddings(images)\n",
    "print(f\"Vision embedding shape (with positional info): {vision_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5c112-68d1-4d17-af6d-d3becb3a99cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "336 / 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b75f6",
   "metadata": {},
   "source": [
    "### 5.2 Image Feature Extraction\n",
    "\n",
    "Let's extract and analyze image features from the vision tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e2bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the image for the model\n",
    "images = [image]\n",
    "images_tensor = process_images(images, image_processor, model.config)\n",
    "if isinstance(images_tensor, list):\n",
    "    images_tensor = [img.to(model.device, dtype=torch.float32) for img in images_tensor]\n",
    "else:\n",
    "    images_tensor = images_tensor.to(model.device, dtype=torch.float32)\n",
    "\n",
    "# Extract image features using the vision tower\n",
    "with torch.no_grad():\n",
    "    image_features = model.model.vision_tower(images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed038a-23f5-40bb-808f-2a3cacce5be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "getattr(model.config, \"image_aspect_ratio\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ab93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the image through the full vision tower\n",
    "emb = model.model.vision_tower(images_tensor)\n",
    "print(f\"Vision tower output shape: {emb.shape}\")\n",
    "# This represents the image features extracted by the vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e75f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze the distribution of values in the image features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    emb.detach().cpu().numpy().flatten(),\n",
    "    log=True,\n",
    "    bins=200,\n",
    ")\n",
    "plt.title(\"Distribution of Vision Features\")\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Log Count\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe9c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the image embeddings\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(\n",
    "    emb.detach().cpu().numpy()[0, :, :],\n",
    "    vmin=-5,\n",
    "    vmax=5,\n",
    "    cmap=\"bwr\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "plt.colorbar(label=\"Feature Value\")\n",
    "plt.title(\"Vision Tower Output Features\")\n",
    "plt.xlabel(\"Feature Dimension\")\n",
    "plt.ylabel(\"Image Patch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e07b99",
   "metadata": {},
   "source": [
    "## 6. Multimodal Integration Analysis\n",
    "\n",
    "Now, let's examine how visual and textual information are aligned and integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f5e24",
   "metadata": {},
   "source": [
    "### 6.1 Vision-Language Projection\n",
    "\n",
    "The model uses a projection layer to map visual features into the same space as text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bae139-412e-4289-a72b-2ae953906758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map image features to text token space with the projector\n",
    "with torch.no_grad():\n",
    "    projected_features = model.model.mm_projector(image_features)\n",
    "\n",
    "# Create a function to find the closest token for each embedding\n",
    "def find_closest_tokens(embeddings, token_embeddings, top_k=1):\n",
    "    # Compute cosine similarity between embeddings and token embeddings\n",
    "    normalized_embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "    normalized_token_embeddings = F.normalize(token_embeddings, p=2, dim=-1)\n",
    "    similarities = torch.matmul(normalized_embeddings, normalized_token_embeddings.T)\n",
    "    \n",
    "    # Get the top-k token indices with highest similarity\n",
    "    if top_k == 1:\n",
    "        closest_token_indices = similarities.argmax(dim=-1)\n",
    "        return closest_token_indices\n",
    "    else:\n",
    "        top_k_values, top_k_indices = torch.topk(similarities, k=top_k, dim=-1)\n",
    "        return top_k_indices, top_k_values\n",
    "    \n",
    "# Get the token embeddings from the model\n",
    "token_embeddings = model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601567d-65f4-4f33-b154-50bc5760f8ce",
   "metadata": {},
   "source": [
    "#### test it with text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a24f61-399b-4929-89bc-448588f38625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_ids \n",
    "prompt = \"Tell me a joke about programming.\"\n",
    "stop_str = \"<image>\"\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "embedded_tokens = model.model.embed_tokens(input_ids)\n",
    "embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cc564-683f-405d-be3f-7c72ae2ffe84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the closest token for each patch embedding\n",
    "closest_token_indices = find_closest_tokens(embedded_tokens, token_embeddings)\n",
    "\n",
    "# Decode the tokens\n",
    "closest_tokens = [tokenizer.decode(idx.item()).strip() for idx in closest_token_indices]\n",
    "closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d4bc7-6c98-45ea-b356-18255f7d70b0",
   "metadata": {},
   "source": [
    "#### test it with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f4acb-68f1-4053-8b91-cf3357360cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the closest token for each patch embedding\n",
    "closest_token_indices = find_closest_tokens(projected_features[0, :], token_embeddings)\n",
    "\n",
    "# Decode the tokens\n",
    "closest_tokens = [tokenizer.decode(idx.item()).strip() for idx in closest_token_indices]\n",
    "closest_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b1491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project vision features to language embedding space\n",
    "mm_emb = model.model.mm_projector(emb)\n",
    "print(f\"After projection: {mm_emb.shape}\")\n",
    "print(f\"Compare with text embedding shape: {embedded_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f77ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze the distribution of values after projection\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    mm_emb.detach().cpu().numpy().flatten(),\n",
    "    log=True,\n",
    "    bins=200,\n",
    ")\n",
    "plt.title(\"Distribution of Projected Vision Features\")\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Log Count\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62eb0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the projected image embeddings\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(\n",
    "    mm_emb.detach().cpu().numpy()[0, ::6, :512],  # Sample every 6th patch for clarity\n",
    "    cmap=\"bwr\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "plt.colorbar(label=\"Feature Value\")\n",
    "plt.title(\"Vision Features After Projection to Language Space\")\n",
    "plt.xlabel(\"Feature Dimension (first 512)\")\n",
    "plt.ylabel(\"Image Patch (sampled)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da37ee",
   "metadata": {},
   "source": [
    "### 6.2 Vision-Language Alignment\n",
    "\n",
    "To understand the alignment between vision and language, we can map image features to the closest text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc592876-5c9e-4e1f-8cd6-24b3858d9741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a helper function to map embeddings back to token IDs\n",
    "# This helps us understand what embedding vectors \"mean\" in token space\n",
    "def embedding_to_token_id(embedding, embed_tokens):\n",
    "    \"\"\"\n",
    "    Maps embedding vectors to token IDs by finding closest tokens in the embedding space.\n",
    "    \n",
    "    Args:\n",
    "        embedding: The embedding vectors to map back to tokens\n",
    "        embed_tokens: The embedding layer that maps tokens to vectors\n",
    "    \n",
    "    Returns:\n",
    "        Token IDs that are closest to the given embeddings in vector space\n",
    "    \"\"\"\n",
    "    # Get the embedding weight matrix (shape: vocab_size x embedding_dim)\n",
    "    weight = embed_tokens.weight  # shape: (vocab_size, embedding_dim)\n",
    "    \n",
    "    # If embedding is a single vector, add batch dimension\n",
    "    if embedding.dim() == 1:\n",
    "        embedding = embedding.unsqueeze(0)\n",
    "    \n",
    "    # Compute cosine similarity between the embedding and all rows in the weight matrix\n",
    "    # Normalize embeddings along the embedding dimension\n",
    "    normalized_embedding = F.normalize(embedding, p=2, dim=-1)\n",
    "    normalized_weight = F.normalize(weight, p=2, dim=-1)\n",
    "    similarities = torch.matmul(normalized_embedding, normalized_weight.T)  # shape: (batch_size, vocab_size)\n",
    "    \n",
    "    # Get the token id with the highest similarity for each embedding in the batch\n",
    "    token_ids = similarities.argmax(dim=-1)\n",
    "    \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca3223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map projected vision features back to token IDs to see \"what the image says\"\n",
    "token_ids_recovered = embedding_to_token_id(mm_emb, model.model.embed_tokens)\n",
    "\n",
    "# Display some of the recovered tokens\n",
    "print(\"Image 'translated' to text (full):\")\n",
    "print(tokenizer.decode(token_ids_recovered[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a73ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample some tokens to see what the model \"sees\" in the image\n",
    "print(\"Sample tokens from image (every 10th patch):\")\n",
    "print(\"-\" * 50)\n",
    "for token_id in token_ids_recovered[0][::100]:\n",
    "    if token_id < 0:\n",
    "        print(f\"Token {token_id:5}: RESERVED_TOKEN\")\n",
    "        continue\n",
    "    print(f\"Token {token_id:5}: '{tokenizer.decode(token_id)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deb909",
   "metadata": {},
   "source": [
    "## 7. Text Embedding Analysis\n",
    "\n",
    "Let's analyze how the model embeds text tokens and processes them through its layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da895b",
   "metadata": {},
   "source": [
    "### 7.1 Token Embedding\n",
    "\n",
    "First, we'll examine the initial embedding of input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b6693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the token embeddings from the embedding layer\n",
    "embedded_tokens = model.model.embed_tokens(input_ids)\n",
    "print(f\"Token embedding shape: {embedded_tokens.shape}\")\n",
    "# The shape is (batch_size, sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097fea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a helper function to map embeddings back to token IDs\n",
    "# This helps us understand what embedding vectors \"mean\" in token space\n",
    "def embedding_to_token_id(embedding, embed_tokens):\n",
    "    \"\"\"\n",
    "    Maps embedding vectors to token IDs by finding closest tokens in the embedding space.\n",
    "    \n",
    "    Args:\n",
    "        embedding: The embedding vectors to map back to tokens\n",
    "        embed_tokens: The embedding layer that maps tokens to vectors\n",
    "    \n",
    "    Returns:\n",
    "        Token IDs that are closest to the given embeddings in vector space\n",
    "    \"\"\"\n",
    "    # Get the embedding weight matrix (shape: vocab_size x embedding_dim)\n",
    "    weight = embed_tokens.weight  # shape: (vocab_size, embedding_dim)\n",
    "    \n",
    "    # If embedding is a single vector, add batch dimension\n",
    "    if embedding.dim() == 1:\n",
    "        embedding = embedding.unsqueeze(0)\n",
    "    \n",
    "    # Compute cosine similarity between the embedding and all rows in the weight matrix\n",
    "    # Normalize embeddings along the embedding dimension\n",
    "    normalized_embedding = F.normalize(embedding, p=2, dim=-1)\n",
    "    normalized_weight = F.normalize(weight, p=2, dim=-1)\n",
    "    similarities = torch.matmul(normalized_embedding, normalized_weight.T)  # shape: (batch_size, vocab_size)\n",
    "    \n",
    "    # Get the token id with the highest similarity for each embedding in the batch\n",
    "    token_ids = similarities.argmax(dim=-1)\n",
    "    \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba056a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map token embeddings back to tokens to verify our understanding\n",
    "token_ids_recovered = embedding_to_token_id(embedded_tokens, model.model.embed_tokens)\n",
    "\n",
    "# Print the tokens to see if they match our original input\n",
    "print(\"Recovered tokens from embeddings:\")\n",
    "print(tokenizer.decode(token_ids_recovered[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469253c9-d397-46b8-a2a6-6e7656a8f634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2297383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the token embeddings\n",
    "plt.figure(figsize=(16, 2))\n",
    "plt.imshow(\n",
    "    embedded_tokens.detach().cpu().numpy()[:,:512],  # Take first 512 dimensions of each token embedding\n",
    "    cmap=\"bwr\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "plt.colorbar(label=\"Embedding Value\")\n",
    "plt.title(\"Token Embedding Visualization\")\n",
    "plt.xlabel(\"Embedding Dimension (first 512)\")\n",
    "plt.ylabel(\"Token Position\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adaceaa",
   "metadata": {},
   "source": [
    "### 7.2 Layer-by-Layer Processing\n",
    "\n",
    "Let's trace how the token representations change as they pass through the model's transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab2bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass the embeddings through each layer of the model\n",
    "enc_output = embedded_tokens\n",
    "print(\"Tracing embeddings through model layers:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, layer in enumerate(model.model.layers, start=1):\n",
    "    enc_output = layer(enc_output.unsqueeze(0))[0]\n",
    "    print(f\"Layer {i}: Shape = {enc_output.shape}\")\n",
    "    \n",
    "    # Optionally visualize intermediate layer outputs\n",
    "    if i % 8 == 0:  # Visualize every 8th layer\n",
    "        plt.figure(figsize=(16, 2))\n",
    "        plt.imshow(\n",
    "            enc_output.detach().cpu().numpy()[0, :, :512], \n",
    "            cmap=\"bwr\",\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "        plt.colorbar(label=\"Activation Value\")\n",
    "        plt.title(f\"Layer {i} Output\")\n",
    "        plt.xlabel(\"Hidden Dimension (first 512)\")\n",
    "        plt.ylabel(\"Token Position\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf32f4",
   "metadata": {},
   "source": [
    "## 8. Text Generation Demo\n",
    "\n",
    "Now we'll demonstrate the model's text generation capabilities:\n",
    "1. Setting generation parameters\n",
    "2. Using a streamer for real-time output\n",
    "3. Generating text based on our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7a32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# Generation parameters\n",
    "temperature = 0.1    # Lower values make output more deterministic (less random)\n",
    "top_p = 0.95         # Nucleus sampling parameter (higher = more diversity)\n",
    "max_new_tokens = 256  # Maximum number of tokens to generate\n",
    "\n",
    "# Create a streamer for generating text progressively\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,      # Don't include the prompt in the output\n",
    "    skip_special_tokens=True,  # Don't include special tokens in the output\n",
    "    timeout=150,           # Timeout in seconds\n",
    ")\n",
    "\n",
    "# Generate text using the model\n",
    "print(\"Generating response to:\", prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with torch.inference_mode():  # Disables gradient computation for inference\n",
    "    # Start generation in a separate thread\n",
    "    thread = Thread(target=model.generate, kwargs=dict(\n",
    "        inputs=input_ids,\n",
    "        do_sample=True,           # Use sampling instead of greedy decoding\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=streamer,\n",
    "        use_cache=True,           # Use KV cache for faster generation\n",
    "        # images=images,          # Uncomment to use image input\n",
    "    ))\n",
    "    thread.start()\n",
    "\n",
    "    # Collect generated text from the streamer\n",
    "    print(\"Generated text:\")\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        # Stop if we encounter the stop string\n",
    "        if generated_text.endswith(stop_str):\n",
    "            generated_text = generated_text[:-len(stop_str)]\n",
    "        print(new_text, end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bce3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the thread\n",
    "thread.join()\n",
    "del thread\n",
    "print(\"Generation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839799b3",
   "metadata": {},
   "source": [
    "## 9. Visualize Image Patches with Closest Text Tokens\n",
    "\n",
    "Let's create a visualization that shows the input image with the closest text tokens written on top of each patch. This helps us understand how the vision-language model processes and interprets different regions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e11855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the closest text tokens on the image patches\n",
    "def visualize_tokens_on_patches(image, tokens, patch_size=14, grid_size=None):\n",
    "    # If grid_size is not provided, calculate it from the number of tokens\n",
    "    if grid_size is None:\n",
    "        # The sqrt of the number of tokens gives us the grid size\n",
    "        # (assuming square patches)\n",
    "        grid_size = int(np.sqrt(len(tokens)))\n",
    "    \n",
    "    # Resize the image to match the grid size * patch_size\n",
    "    target_size = (grid_size * patch_size, grid_size * patch_size)\n",
    "    resized_img = image.resize(target_size, Image.LANCZOS)\n",
    "    \n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    ax.imshow(resized_img)\n",
    "    \n",
    "    # Plot the tokens on top of each patch\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            token_idx = i * grid_size + j\n",
    "            if token_idx < len(tokens):\n",
    "                token = tokens[token_idx]\n",
    "                # Limit token display length to avoid overcrowding\n",
    "                if len(token) > 5:\n",
    "                    token = token[:4] + '...'\n",
    "                ax.text(j * patch_size + patch_size // 2, i * patch_size + patch_size // 2, \n",
    "                        token, color='white', fontsize=8, ha='center', va='center',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.2\", fc='black', alpha=0.5))\n",
    "    \n",
    "    ax.set_title(\"Image Patches with Closest Text Tokens\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get the vision model configuration to determine patch size and grid size\n",
    "vision_config = model.model.vision_tower.vision_tower.config\n",
    "patch_size = vision_config.patch_size\n",
    "image_size = vision_config.image_size\n",
    "grid_size = image_size // patch_size\n",
    "\n",
    "# Visualize the tokens on the image\n",
    "visualize_tokens_on_patches(image, closest_tokens, patch_size=patch_size, grid_size=grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e4209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a more detailed visualization with top-3 tokens and their probabilities\n",
    "def visualize_top_k_tokens_on_patches(image, token_embeddings, patch_embeddings, tokenizer, \n",
    "                                   patch_size=14, grid_size=None, top_k=3):\n",
    "    if grid_size is None:\n",
    "        grid_size = int(np.sqrt(len(patch_embeddings)))\n",
    "    \n",
    "    # Get top-k tokens for each patch\n",
    "    top_indices, top_scores = find_closest_tokens(patch_embeddings, token_embeddings, top_k=top_k)\n",
    "    \n",
    "    # Convert scores to probabilities via softmax\n",
    "    top_probs = F.softmax(top_scores, dim=-1)\n",
    "    \n",
    "    # Decode the top tokens\n",
    "    top_tokens = []\n",
    "    for i in range(len(top_indices)):\n",
    "        tokens = [tokenizer.decode(idx.item()).strip() for idx in top_indices[i]]\n",
    "        probs = top_probs[i].tolist()\n",
    "        top_tokens.append(list(zip(tokens, probs)))\n",
    "    \n",
    "    # Resize the image to match the grid\n",
    "    target_size = (grid_size * patch_size, grid_size * patch_size)\n",
    "    resized_img = image.resize(target_size, Image.LANCZOS)\n",
    "    \n",
    "    # Create a figure for visualization\n",
    "    fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    ax.imshow(resized_img)\n",
    "    \n",
    "    # Plot the top tokens on each patch\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            patch_idx = i * grid_size + j\n",
    "            if patch_idx < len(top_tokens):\n",
    "                token_info = top_tokens[patch_idx]\n",
    "                # Format the text: token (prob%)\n",
    "                text = '\\n'.join([f\"{t[:4]}.. ({p:.0%})\" if len(t) > 5 else f\"{t} ({p:.0%})\" \n",
    "                                  for t, p in token_info])\n",
    "                ax.text(j * patch_size + patch_size // 2, i * patch_size + patch_size // 2, \n",
    "                       text, color='white', fontsize=7, ha='center', va='center',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.2\", fc='black', alpha=0.6))\n",
    "    \n",
    "    ax.set_title(f\"Image Patches with Top-{top_k} Closest Text Tokens and Their Probabilities\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get top-3 tokens for each patch\n",
    "with torch.no_grad():\n",
    "    visualize_top_k_tokens_on_patches(\n",
    "        image, \n",
    "        token_embeddings,\n",
    "        projected_features[0, 1:],\n",
    "        tokenizer,\n",
    "        patch_size=patch_size,\n",
    "        grid_size=grid_size,\n",
    "        top_k=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79465c9b",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook has explored the ShareGPT4V-7B model's architecture and capabilities:\n",
    "\n",
    "1. **Model Structure**: The model combines a LLaMA-based language model with a vision transformer\n",
    "2. **Text Processing**: Text is tokenized and embedded into a high-dimensional space\n",
    "3. **Image Processing**: Images are divided into patches and processed through a vision transformer\n",
    "4. **Multimodal Integration**: Visual features are projected into the language embedding space\n",
    "5. **Generation**: The model can generate text based on either text or text+image inputs\n",
    "\n",
    "The key to multimodal capabilities is the projection of visual features into the language embedding space, allowing the model to process both modalities coherently.\n",
    "\n",
    "Further explorations could include:\n",
    "- Analyzing attention patterns between image and text tokens\n",
    "- Testing the model's performance on various visual reasoning tasks\n",
    "- Examining cross-modal transfer and emergent capabilities"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-poetry-kernel",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "conda-base-poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
